{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "import pandas as pd\n",
    "import spacy\n",
    "import os\n",
    "import sys\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import pickle as pkl\n",
    "import random\n",
    "\n",
    "# Which tokenizer to use? TweetTokenizer is more robust than the vanilla tokenizer, but then,\n",
    "# will the intelligence of tokenization matter in the long run when trained using DL?\n",
    "from nltk.tokenize import word_tokenize, TweetTokenizer\n",
    "tokenizer = TweetTokenizer(preserve_case = False)\n",
    "\n",
    "from gensim.models import Word2Vec, KeyedVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = \"../data/\"\n",
    "cornell_folder = os.path.join(base_path, \"cornell movie-dialogs corpus\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../data/cornell movie-dialogs corpus/movie_lines.txt'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movie_lines = os.path.join(cornell_folder, \"movie_lines.txt\")\n",
    "movie_lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_lines_features = [\"LineID\", \"Character\", \"Movie\", \"Name\", \"Line\"]\n",
    "movie_lines = pd.read_csv(movie_lines, sep = \"\\+\\+\\+\\$\\+\\+\\+\", engine = \"python\", index_col = False, names = movie_lines_features)\n",
    "\n",
    "# Using only the required columns, namely, \"LineID\" and \"Line\"\n",
    "movie_lines = movie_lines[[\"LineID\", \"Line\"]]\n",
    "\n",
    "# Strip the space from \"LineID\" for further usage and change the datatype of \"Line\"\n",
    "movie_lines[\"LineID\"] = movie_lines[\"LineID\"].apply(str.strip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LineID</th>\n",
       "      <th>Line</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>L1045</td>\n",
       "      <td>They do not!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>L1044</td>\n",
       "      <td>They do to!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>L985</td>\n",
       "      <td>I hope so.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>L984</td>\n",
       "      <td>She okay?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>L925</td>\n",
       "      <td>Let's go.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  LineID           Line\n",
       "0  L1045   They do not!\n",
       "1  L1044    They do to!\n",
       "2   L985     I hope so.\n",
       "3   L984      She okay?\n",
       "4   L925      Let's go."
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movie_lines.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_conversations_file = os.path.join(cornell_folder, \"movie_conversations.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_conversations_features = [\"Character1\", \"Character2\", \"Movie\", \"Conversation\"]\n",
    "movie_conversations = pd.read_csv(movie_conversations_file, sep = \"\\+\\+\\+\\$\\+\\+\\+\", engine = \"python\", index_col = False, names = movie_conversations_features)\n",
    "\n",
    "# Again using the required feature, \"Conversation\"\n",
    "movie_conversations = movie_conversations[\"Conversation\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     ['L194', 'L195', 'L196', 'L197']\n",
       "1                     ['L198', 'L199']\n",
       "2     ['L200', 'L201', 'L202', 'L203']\n",
       "3             ['L204', 'L205', 'L206']\n",
       "4                     ['L207', 'L208']\n",
       "Name: Conversation, dtype: object"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movie_conversations.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 83097/83097 [1:41:36<00:00, 13.63it/s]  \n"
     ]
    }
   ],
   "source": [
    "conversation = [[str(list(movie_lines.loc[movie_lines[\"LineID\"] == u.strip().strip(\"'\"), \"Line\"])[0]).strip() \n",
    "                 for u in c.strip().strip('[').strip(']').split(',')] for c in tqdm(movie_conversations)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Why?',\n",
       " 'Unsolved mystery.  She used to be really popular when she started high school, then it was just like she got sick of it or something.',\n",
       " \"That's a shame.\"]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pkl\n",
    "with open(\"conversations.pkl\", \"wb\") as handle:\n",
    "    pkl.dump(conversation, handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"conversations.pkl\", \"rb\") as handle:\n",
    "    conversation = pkl.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "83097"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(conversation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    83097.000000\n",
       "mean         3.666955\n",
       "std          2.891798\n",
       "min          2.000000\n",
       "25%          2.000000\n",
       "50%          3.000000\n",
       "75%          4.000000\n",
       "max         89.000000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate the dialogue length statistics\n",
    "\n",
    "dialogue_lengths = [len(dialogue) for dialogue in conversation]\n",
    "pd.Series(dialogue_lengths).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "# Generate 50 sample pairs - 14/03/2019\n",
    "indices = random.sample(range(len(conversation)), 50)\n",
    "sample_context_list = []\n",
    "sample_response_list = []\n",
    "\n",
    "for index in indices:\n",
    "    \n",
    "    response = conversation[index][-1]\n",
    "        \n",
    "    context = \"FS: \" + conversation[index][0] + \"\\n\"\n",
    "    for i in range(1, len(conversation[index]) - 1):\n",
    "        \n",
    "        if i % 2 == 0:\n",
    "            prefix = \"FS: \"\n",
    "        else:\n",
    "            prefix = \"SS: \"\n",
    "            \n",
    "        context += prefix + conversation[index][i] + \"\\n\"\n",
    "        \n",
    "    sample_context_list.append(context)\n",
    "    sample_response_list.append(response)\n",
    "\n",
    "with open(\"cornell_movie_dialogue_sample.csv\", \"w\") as handle:\n",
    "    for c, r in zip(sample_context_list, sample_response_list):\n",
    "        handle.write('\"' + c + '\"' + \"#\" + r + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_pairs(conversation):\n",
    "    \n",
    "    context_list = []\n",
    "    response_list = []\n",
    "    \n",
    "    for dialogue in tqdm(conversation):\n",
    "        \n",
    "        response = dialogue[-1]\n",
    "        \n",
    "        context = dialogue[0]\n",
    "        for index in range(1, len(dialogue) - 1):\n",
    "            context += dialogue[index]\n",
    "        \n",
    "        context_list.append(context)\n",
    "        response_list.append(response)\n",
    "        \n",
    "    return context_list, response_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 83097/83097 [00:00<00:00, 425083.55it/s]\n"
     ]
    }
   ],
   "source": [
    "context_list, response_list = generate_pairs(conversation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/alditopalli/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_docs = context_list\n",
    "target_docs = response_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"input_docs.pkl\", \"wb\") as handle:\n",
    "    pkl.dump(input_docs, handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"target_docs.pkl\", \"wb\") as handle:\n",
    "    pkl.dump(target_docs, handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'my': 1, 'name': 2, 'is': 3, 'aldi': 4, 'john': 5, 'albert': 6}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "tokenizer = Tokenizer(num_words=2)\n",
    "texts = [\"my name is aldi\", \"my name is john\", \"my name is albert\"]\n",
    "tokenizer.num_words = 2\n",
    "tokenizer.fit_on_texts(texts)\n",
    "tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 2, 3], [1, 2, 3], [1, 2, 3]]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.num_words = 4\n",
    "tokenizer.texts_to_sequences(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'my': 1, 'name': 2, 'is': 3, 'aldi': 4, 'john': 5, 'albert': 6}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 2, 3], [1, 2, 3], [1, 2, 3]]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.texts_to_sequences(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy_langdetect import LanguageDetector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.add_pipe(LanguageDetector(), name='language_detector', last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'en'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"muy bin, me voy a hacer la comida.\"\n",
    "text1 = \"Hey there whats up.\"\n",
    "doc = nlp(text1)\n",
    "\n",
    "doc._.language['language']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting contractions\n",
      "  Downloading contractions-0.0.45-py2.py3-none-any.whl (6.4 kB)\n",
      "Collecting textsearch\n",
      "  Downloading textsearch-0.0.17-py2.py3-none-any.whl (7.5 kB)\n",
      "Collecting Unidecode\n",
      "  Downloading Unidecode-1.1.2-py2.py3-none-any.whl (239 kB)\n",
      "\u001b[K     |████████████████████████████████| 239 kB 2.3 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting pyahocorasick\n",
      "  Downloading pyahocorasick-1.4.0.tar.gz (312 kB)\n",
      "\u001b[K     |████████████████████████████████| 312 kB 2.8 MB/s eta 0:00:01\n",
      "\u001b[?25hBuilding wheels for collected packages: pyahocorasick\n",
      "  Building wheel for pyahocorasick (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pyahocorasick: filename=pyahocorasick-1.4.0-cp38-cp38-macosx_10_15_x86_64.whl size=31047 sha256=73c2221c0c3477a55f67b48a69a1c80fb8b2e0bd28f39fa8dfd33c1a557c89bc\n",
      "  Stored in directory: /Users/alditopalli/Library/Caches/pip/wheels/d0/72/5e/5af4d48e71e0b4dc197445664eb1719fe9c9b6fcead4930595\n",
      "Successfully built pyahocorasick\n",
      "Installing collected packages: Unidecode, pyahocorasick, textsearch, contractions\n",
      "Successfully installed Unidecode-1.1.2 contractions-0.0.45 pyahocorasick-1.4.0 textsearch-0.0.17\n"
     ]
    }
   ],
   "source": [
    "!pip3 install contractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import contractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on package contractions:\n",
      "\n",
      "NAME\n",
      "    contractions\n",
      "\n",
      "PACKAGE CONTENTS\n",
      "    data (package)\n",
      "    test___init__\n",
      "\n",
      "FUNCTIONS\n",
      "    add(key, value)\n",
      "    \n",
      "    fix(s, leftovers=True, slang=True)\n",
      "    \n",
      "    get_combinations(tokens, joiners)\n",
      "    \n",
      "    intersperse(lst, item)\n",
      "\n",
      "DATA\n",
      "    comb = \"you'd've\"\n",
      "    contractions_dict = {\"'cause\": 'because', \"'tis\": 'it is', \"'twas\": 'i...\n",
      "    json_open = b'{\\n    \"ima\": \"I am going to\",\\n    \"gonna\": \"...ssible\"...\n",
      "    k = 'you’d’ve'\n",
      "    leftovers_dict = {\"'all\": '', \"'am\": '', \"'cause\": 'because', \"'coz\": ...\n",
      "    month = 'december'\n",
      "    replacers = {(False, False): TextSearch(case='ignore', returns='norm',...\n",
      "    safety_keys = {\"he'll\", \"he's\", \"i'd\", \"i'll\", \"it's\", \"we'd\", ...}\n",
      "    slang_dict = {\"'cause\": 'because', \"'tis\": 'it is', \"'twas\": 'it was',...\n",
      "    tokens = ['you', 'd', 've']\n",
      "    ts_basic = TextSearch(case='ignore', returns='norm', num_items=329)\n",
      "    ts_leftovers = TextSearch(case='ignore', returns='norm', num_items=357...\n",
      "    ts_leftovers_slang = TextSearch(case='ignore', returns='norm', num_ite...\n",
      "    ts_slang = TextSearch(case='ignore', returns='norm', num_items=555)\n",
      "    unsafe_dict = {\"'cause\": 'because', \"'tis\": 'it is', \"'twas\": 'it was'...\n",
      "    v = 'you would have'\n",
      "\n",
      "FILE\n",
      "    /usr/local/lib/python3.8/site-packages/contractions/__init__.py\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(contractions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\"I'm\": 'I am',\n",
       " \"I'm'a\": 'I am about to',\n",
       " \"I'm'o\": 'I am going to',\n",
       " \"I've\": 'I have',\n",
       " \"I'll\": 'I will',\n",
       " \"I'll've\": 'I will have',\n",
       " \"I'd\": 'I would',\n",
       " \"I'd've\": 'I would have',\n",
       " 'Whatcha': 'What are you',\n",
       " \"amn't\": 'am not',\n",
       " \"ain't\": 'are not',\n",
       " \"aren't\": 'are not',\n",
       " \"'cause\": 'because',\n",
       " \"can't\": 'can not',\n",
       " \"can't've\": 'can not have',\n",
       " \"could've\": 'could have',\n",
       " \"couldn't\": 'could not',\n",
       " \"couldn't've\": 'could not have',\n",
       " \"daren't\": 'dare not',\n",
       " \"daresn't\": 'dare not',\n",
       " \"dasn't\": 'dare not',\n",
       " \"didn't\": 'did not',\n",
       " 'didn’t': 'did not',\n",
       " \"don't\": 'do not',\n",
       " 'don’t': 'do not',\n",
       " \"doesn't\": 'does not',\n",
       " \"e'er\": 'ever',\n",
       " \"everyone's\": 'everyone is',\n",
       " 'finna': 'fixing to',\n",
       " 'gimme': 'give me',\n",
       " \"gon't\": 'go not',\n",
       " 'gonna': 'going to',\n",
       " 'gotta': 'got to',\n",
       " \"hadn't\": 'had not',\n",
       " \"hadn't've\": 'had not have',\n",
       " \"hasn't\": 'has not',\n",
       " \"haven't\": 'have not',\n",
       " \"he've\": 'he have',\n",
       " \"he's\": 'he is',\n",
       " \"he'll\": 'he will',\n",
       " \"he'll've\": 'he will have',\n",
       " \"he'd\": 'he would',\n",
       " \"he'd've\": 'he would have',\n",
       " \"here's\": 'here is',\n",
       " \"how're\": 'how are',\n",
       " \"how'd\": 'how did',\n",
       " \"how'd'y\": 'how do you',\n",
       " \"how's\": 'how is',\n",
       " \"how'll\": 'how will',\n",
       " \"isn't\": 'is not',\n",
       " \"it's\": 'it is',\n",
       " \"'tis\": 'it is',\n",
       " \"'twas\": 'it was',\n",
       " \"it'll\": 'it will',\n",
       " \"it'll've\": 'it will have',\n",
       " \"it'd\": 'it would',\n",
       " \"it'd've\": 'it would have',\n",
       " 'kinda': 'kind of',\n",
       " \"let's\": 'let us',\n",
       " 'luv': 'love',\n",
       " \"ma'am\": 'madam',\n",
       " \"may've\": 'may have',\n",
       " \"mayn't\": 'may not',\n",
       " \"might've\": 'might have',\n",
       " \"mightn't\": 'might not',\n",
       " \"mightn't've\": 'might not have',\n",
       " \"must've\": 'must have',\n",
       " \"mustn't\": 'must not',\n",
       " \"mustn't've\": 'must not have',\n",
       " \"needn't\": 'need not',\n",
       " \"needn't've\": 'need not have',\n",
       " \"ne'er\": 'never',\n",
       " \"o'\": 'of',\n",
       " \"o'clock\": 'of the clock',\n",
       " \"ol'\": 'old',\n",
       " \"oughtn't\": 'ought not',\n",
       " \"oughtn't've\": 'ought not have',\n",
       " \"o'er\": 'over',\n",
       " \"shan't\": 'shall not',\n",
       " \"sha'n't\": 'shall not',\n",
       " \"shalln't\": 'shall not',\n",
       " \"shan't've\": 'shall not have',\n",
       " \"she's\": 'she is',\n",
       " \"she'll\": 'she will',\n",
       " \"she'd\": 'she would',\n",
       " \"she'd've\": 'she would have',\n",
       " \"should've\": 'should have',\n",
       " \"shouldn't\": 'should not',\n",
       " \"shouldn't've\": 'should not have',\n",
       " \"so've\": 'so have',\n",
       " \"so's\": 'so is',\n",
       " \"somebody's\": 'somebody is',\n",
       " \"someone's\": 'someone is',\n",
       " \"something's\": 'something is',\n",
       " 'sux': 'sucks',\n",
       " \"that're\": 'that are',\n",
       " \"that's\": 'that is',\n",
       " \"that'll\": 'that will',\n",
       " \"that'd\": 'that would',\n",
       " \"that'd've\": 'that would have',\n",
       " 'em': 'them',\n",
       " \"there're\": 'there are',\n",
       " \"there's\": 'there is',\n",
       " \"there'll\": 'there will',\n",
       " \"there'd\": 'there would',\n",
       " \"there'd've\": 'there would have',\n",
       " \"these're\": 'these are',\n",
       " \"they're\": 'they are',\n",
       " \"they've\": 'they have',\n",
       " \"they'll\": 'they will',\n",
       " \"they'll've\": 'they will have',\n",
       " \"they'd\": 'they would',\n",
       " \"they'd've\": 'they would have',\n",
       " \"this's\": 'this is',\n",
       " \"those're\": 'those are',\n",
       " \"to've\": 'to have',\n",
       " 'wanna': 'want to',\n",
       " \"wasn't\": 'was not',\n",
       " \"we're\": 'we are',\n",
       " \"we've\": 'we have',\n",
       " \"we'll\": 'we will',\n",
       " \"we'll've\": 'we will have',\n",
       " \"we'd\": 'we would',\n",
       " \"we'd've\": 'we would have',\n",
       " \"weren't\": 'were not',\n",
       " \"what're\": 'what are',\n",
       " \"what'd\": 'what did',\n",
       " \"what've\": 'what have',\n",
       " \"what's\": 'what is',\n",
       " \"what'll\": 'what will',\n",
       " \"what'll've\": 'what will have',\n",
       " \"when've\": 'when have',\n",
       " \"when's\": 'when is',\n",
       " \"where're\": 'where are',\n",
       " \"where'd\": 'where did',\n",
       " \"where've\": 'where have',\n",
       " \"where's\": 'where is',\n",
       " \"which's\": 'which is',\n",
       " \"who're\": 'who are',\n",
       " \"who've\": 'who have',\n",
       " \"who's\": 'who is',\n",
       " \"who'll\": 'who will',\n",
       " \"who'll've\": 'who will have',\n",
       " \"who'd\": 'who would',\n",
       " \"who'd've\": 'who would have',\n",
       " \"why're\": 'why are',\n",
       " \"why'd\": 'why did',\n",
       " \"why've\": 'why have',\n",
       " \"why's\": 'why is',\n",
       " \"will've\": 'will have',\n",
       " \"won't\": 'will not',\n",
       " \"won't've\": 'will not have',\n",
       " \"would've\": 'would have',\n",
       " \"wouldn't\": 'would not',\n",
       " \"wouldn't've\": 'would not have',\n",
       " \"y'all\": 'you all',\n",
       " \"y'all're\": 'you all are',\n",
       " \"y'all've\": 'you all have',\n",
       " \"y'all'd\": 'you all would',\n",
       " \"y'all'd've\": 'you all would have',\n",
       " \"you're\": 'you are',\n",
       " \"you've\": 'you have',\n",
       " \"you'll've\": 'you shall have',\n",
       " \"you'll\": 'you will',\n",
       " \"you'd\": 'you would',\n",
       " \"you'd've\": 'you would have',\n",
       " 'jan.': 'january',\n",
       " 'feb.': 'february',\n",
       " 'mar.': 'march',\n",
       " 'apr.': 'april',\n",
       " 'jun.': 'june',\n",
       " 'jul.': 'july',\n",
       " 'aug.': 'august',\n",
       " 'sep.': 'september',\n",
       " 'oct.': 'october',\n",
       " 'nov.': 'november',\n",
       " 'dec.': 'december',\n",
       " 'I’m': 'I am',\n",
       " 'I’m’a': 'I am about to',\n",
       " 'I’m’o': 'I am going to',\n",
       " 'I’ve': 'I have',\n",
       " 'I’ll': 'I will',\n",
       " 'I’ll’ve': 'I will have',\n",
       " 'I’d': 'I would',\n",
       " 'I’d’ve': 'I would have',\n",
       " 'amn’t': 'am not',\n",
       " 'ain’t': 'are not',\n",
       " 'aren’t': 'are not',\n",
       " '’cause': 'because',\n",
       " 'can’t': 'can not',\n",
       " 'can’t’ve': 'can not have',\n",
       " 'could’ve': 'could have',\n",
       " 'couldn’t': 'could not',\n",
       " 'couldn’t’ve': 'could not have',\n",
       " 'daren’t': 'dare not',\n",
       " 'daresn’t': 'dare not',\n",
       " 'dasn’t': 'dare not',\n",
       " 'doesn’t': 'does not',\n",
       " 'e’er': 'ever',\n",
       " 'everyone’s': 'everyone is',\n",
       " 'gon’t': 'go not',\n",
       " 'hadn’t': 'had not',\n",
       " 'hadn’t’ve': 'had not have',\n",
       " 'hasn’t': 'has not',\n",
       " 'haven’t': 'have not',\n",
       " 'he’ve': 'he have',\n",
       " 'he’s': 'he is',\n",
       " 'he’ll': 'he will',\n",
       " 'he’ll’ve': 'he will have',\n",
       " 'he’d': 'he would',\n",
       " 'he’d’ve': 'he would have',\n",
       " 'here’s': 'here is',\n",
       " 'how’re': 'how are',\n",
       " 'how’d': 'how did',\n",
       " 'how’d’y': 'how do you',\n",
       " 'how’s': 'how is',\n",
       " 'how’ll': 'how will',\n",
       " 'isn’t': 'is not',\n",
       " 'it’s': 'it is',\n",
       " '’tis': 'it is',\n",
       " '’twas': 'it was',\n",
       " 'it’ll': 'it will',\n",
       " 'it’ll’ve': 'it will have',\n",
       " 'it’d': 'it would',\n",
       " 'it’d’ve': 'it would have',\n",
       " 'let’s': 'let us',\n",
       " 'ma’am': 'madam',\n",
       " 'may’ve': 'may have',\n",
       " 'mayn’t': 'may not',\n",
       " 'might’ve': 'might have',\n",
       " 'mightn’t': 'might not',\n",
       " 'mightn’t’ve': 'might not have',\n",
       " 'must’ve': 'must have',\n",
       " 'mustn’t': 'must not',\n",
       " 'mustn’t’ve': 'must not have',\n",
       " 'needn’t': 'need not',\n",
       " 'needn’t’ve': 'need not have',\n",
       " 'ne’er': 'never',\n",
       " 'o’': 'of',\n",
       " 'o’clock': 'of the clock',\n",
       " 'ol’': 'old',\n",
       " 'oughtn’t': 'ought not',\n",
       " 'oughtn’t’ve': 'ought not have',\n",
       " 'o’er': 'over',\n",
       " 'shan’t': 'shall not',\n",
       " 'sha’n’t': 'shall not',\n",
       " 'shalln’t': 'shall not',\n",
       " 'shan’t’ve': 'shall not have',\n",
       " 'she’s': 'she is',\n",
       " 'she’ll': 'she will',\n",
       " 'she’d': 'she would',\n",
       " 'she’d’ve': 'she would have',\n",
       " 'should’ve': 'should have',\n",
       " 'shouldn’t': 'should not',\n",
       " 'shouldn’t’ve': 'should not have',\n",
       " 'so’ve': 'so have',\n",
       " 'so’s': 'so is',\n",
       " 'somebody’s': 'somebody is',\n",
       " 'someone’s': 'someone is',\n",
       " 'something’s': 'something is',\n",
       " 'that’re': 'that are',\n",
       " 'that’s': 'that is',\n",
       " 'that’ll': 'that will',\n",
       " 'that’d': 'that would',\n",
       " 'that’d’ve': 'that would have',\n",
       " 'there’re': 'there are',\n",
       " 'there’s': 'there is',\n",
       " 'there’ll': 'there will',\n",
       " 'there’d': 'there would',\n",
       " 'there’d’ve': 'there would have',\n",
       " 'these’re': 'these are',\n",
       " 'they’re': 'they are',\n",
       " 'they’ve': 'they have',\n",
       " 'they’ll': 'they will',\n",
       " 'they’ll’ve': 'they will have',\n",
       " 'they’d': 'they would',\n",
       " 'they’d’ve': 'they would have',\n",
       " 'this’s': 'this is',\n",
       " 'those’re': 'those are',\n",
       " 'to’ve': 'to have',\n",
       " 'wasn’t': 'was not',\n",
       " 'we’re': 'we are',\n",
       " 'we’ve': 'we have',\n",
       " 'we’ll': 'we will',\n",
       " 'we’ll’ve': 'we will have',\n",
       " 'we’d': 'we would',\n",
       " 'we’d’ve': 'we would have',\n",
       " 'weren’t': 'were not',\n",
       " 'what’re': 'what are',\n",
       " 'what’d': 'what did',\n",
       " 'what’ve': 'what have',\n",
       " 'what’s': 'what is',\n",
       " 'what’ll': 'what will',\n",
       " 'what’ll’ve': 'what will have',\n",
       " 'when’ve': 'when have',\n",
       " 'when’s': 'when is',\n",
       " 'where’re': 'where are',\n",
       " 'where’d': 'where did',\n",
       " 'where’ve': 'where have',\n",
       " 'where’s': 'where is',\n",
       " 'which’s': 'which is',\n",
       " 'who’re': 'who are',\n",
       " 'who’ve': 'who have',\n",
       " 'who’s': 'who is',\n",
       " 'who’ll': 'who will',\n",
       " 'who’ll’ve': 'who will have',\n",
       " 'who’d': 'who would',\n",
       " 'who’d’ve': 'who would have',\n",
       " 'why’re': 'why are',\n",
       " 'why’d': 'why did',\n",
       " 'why’ve': 'why have',\n",
       " 'why’s': 'why is',\n",
       " 'will’ve': 'will have',\n",
       " 'won’t': 'will not',\n",
       " 'won’t’ve': 'will not have',\n",
       " 'would’ve': 'would have',\n",
       " 'wouldn’t': 'would not',\n",
       " 'wouldn’t’ve': 'would not have',\n",
       " 'y’all': 'you all',\n",
       " 'y’all’re': 'you all are',\n",
       " 'y’all’ve': 'you all have',\n",
       " 'y’all’d': 'you all would',\n",
       " 'y’all’d’ve': 'you all would have',\n",
       " 'you’re': 'you are',\n",
       " 'you’ve': 'you have',\n",
       " 'you’ll’ve': 'you shall have',\n",
       " 'you’ll': 'you will',\n",
       " 'you’d': 'you would',\n",
       " 'you’d’ve': 'you would have'}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "contractions.contractions_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
