{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "import pandas as pd\n",
    "import spacy\n",
    "import os\n",
    "import sys\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import pickle as pkl\n",
    "import random\n",
    "\n",
    "# Which tokenizer to use? TweetTokenizer is more robust than the vanilla tokenizer, but then,\n",
    "# will the intelligence of tokenization matter in the long run when trained using DL?\n",
    "from nltk.tokenize import word_tokenize, TweetTokenizer\n",
    "tokenizer = TweetTokenizer(preserve_case = False)\n",
    "\n",
    "from gensim.models import Word2Vec, KeyedVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = \"../data/\"\n",
    "cornell_folder = os.path.join(base_path, \"cornell movie-dialogs corpus\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../data/cornell movie-dialogs corpus/movie_lines.txt'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movie_lines = os.path.join(cornell_folder, \"movie_lines.txt\")\n",
    "movie_lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_lines_features = [\"LineID\", \"Character\", \"Movie\", \"Name\", \"Line\"]\n",
    "movie_lines = pd.read_csv(movie_lines, sep = \"\\+\\+\\+\\$\\+\\+\\+\", engine = \"python\", index_col = False, names = movie_lines_features)\n",
    "\n",
    "# Using only the required columns, namely, \"LineID\" and \"Line\"\n",
    "movie_lines = movie_lines[[\"LineID\", \"Line\"]]\n",
    "\n",
    "# Strip the space from \"LineID\" for further usage and change the datatype of \"Line\"\n",
    "movie_lines[\"LineID\"] = movie_lines[\"LineID\"].apply(str.strip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LineID</th>\n",
       "      <th>Line</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>L1045</td>\n",
       "      <td>They do not!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>L1044</td>\n",
       "      <td>They do to!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>L985</td>\n",
       "      <td>I hope so.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>L984</td>\n",
       "      <td>She okay?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>L925</td>\n",
       "      <td>Let's go.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  LineID           Line\n",
       "0  L1045   They do not!\n",
       "1  L1044    They do to!\n",
       "2   L985     I hope so.\n",
       "3   L984      She okay?\n",
       "4   L925      Let's go."
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movie_lines.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_conversations_file = os.path.join(cornell_folder, \"movie_conversations.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_conversations_features = [\"Character1\", \"Character2\", \"Movie\", \"Conversation\"]\n",
    "movie_conversations = pd.read_csv(movie_conversations_file, sep = \"\\+\\+\\+\\$\\+\\+\\+\", engine = \"python\", index_col = False, names = movie_conversations_features)\n",
    "\n",
    "# Again using the required feature, \"Conversation\"\n",
    "movie_conversations = movie_conversations[\"Conversation\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     ['L194', 'L195', 'L196', 'L197']\n",
       "1                     ['L198', 'L199']\n",
       "2     ['L200', 'L201', 'L202', 'L203']\n",
       "3             ['L204', 'L205', 'L206']\n",
       "4                     ['L207', 'L208']\n",
       "Name: Conversation, dtype: object"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movie_conversations.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 83097/83097 [1:41:36<00:00, 13.63it/s]  \n"
     ]
    }
   ],
   "source": [
    "conversation = [[str(list(movie_lines.loc[movie_lines[\"LineID\"] == u.strip().strip(\"'\"), \"Line\"])[0]).strip() \n",
    "                 for u in c.strip().strip('[').strip(']').split(',')] for c in tqdm(movie_conversations)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Why?',\n",
       " 'Unsolved mystery.  She used to be really popular when she started high school, then it was just like she got sick of it or something.',\n",
       " \"That's a shame.\"]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pkl\n",
    "with open(\"conversations.pkl\", \"wb\") as handle:\n",
    "    pkl.dump(conversation, handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"conversations.pkl\", \"rb\") as handle:\n",
    "    conversation = pkl.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "83097"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(conversation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    83097.000000\n",
       "mean         3.666955\n",
       "std          2.891798\n",
       "min          2.000000\n",
       "25%          2.000000\n",
       "50%          3.000000\n",
       "75%          4.000000\n",
       "max         89.000000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate the dialogue length statistics\n",
    "\n",
    "dialogue_lengths = [len(dialogue) for dialogue in conversation]\n",
    "pd.Series(dialogue_lengths).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "# Generate 50 sample pairs - 14/03/2019\n",
    "indices = random.sample(range(len(conversation)), 50)\n",
    "sample_context_list = []\n",
    "sample_response_list = []\n",
    "\n",
    "for index in indices:\n",
    "    \n",
    "    response = conversation[index][-1]\n",
    "        \n",
    "    context = \"FS: \" + conversation[index][0] + \"\\n\"\n",
    "    for i in range(1, len(conversation[index]) - 1):\n",
    "        \n",
    "        if i % 2 == 0:\n",
    "            prefix = \"FS: \"\n",
    "        else:\n",
    "            prefix = \"SS: \"\n",
    "            \n",
    "        context += prefix + conversation[index][i] + \"\\n\"\n",
    "        \n",
    "    sample_context_list.append(context)\n",
    "    sample_response_list.append(response)\n",
    "\n",
    "with open(\"cornell_movie_dialogue_sample.csv\", \"w\") as handle:\n",
    "    for c, r in zip(sample_context_list, sample_response_list):\n",
    "        handle.write('\"' + c + '\"' + \"#\" + r + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_pairs(conversation):\n",
    "    \n",
    "    context_list = []\n",
    "    response_list = []\n",
    "    \n",
    "    for dialogue in tqdm(conversation):\n",
    "        \n",
    "        response = dialogue[-1]\n",
    "        \n",
    "        context = dialogue[0]\n",
    "        for index in range(1, len(dialogue) - 1):\n",
    "            context += dialogue[index]\n",
    "        \n",
    "        context_list.append(context)\n",
    "        response_list.append(response)\n",
    "        \n",
    "    return context_list, response_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 83097/83097 [00:00<00:00, 425083.55it/s]\n"
     ]
    }
   ],
   "source": [
    "context_list, response_list = generate_pairs(conversation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/alditopalli/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_docs = context_list\n",
    "target_docs = response_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"input_docs.pkl\", \"wb\") as handle:\n",
    "    pkl.dump(input_docs, handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"target_docs.pkl\", \"wb\") as handle:\n",
    "    pkl.dump(target_docs, handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'my': 1, 'name': 2, 'is': 3, 'aldi': 4, 'john': 5, 'albert': 6}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "tokenizer = Tokenizer(num_words=2)\n",
    "texts = [\"my name is aldi\", \"my name is john\", \"my name is albert\"]\n",
    "tokenizer.num_words = 2\n",
    "tokenizer.fit_on_texts(texts)\n",
    "tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 2, 3], [1, 2, 3], [1, 2, 3]]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.num_words = 4\n",
    "tokenizer.texts_to_sequences(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'my': 1, 'name': 2, 'is': 3, 'aldi': 4, 'john': 5, 'albert': 6}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 2, 3], [1, 2, 3], [1, 2, 3]]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.texts_to_sequences(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
