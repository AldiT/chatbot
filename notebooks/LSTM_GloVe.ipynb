{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk\n",
      "  Downloading nltk-3.5.zip (1.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.4 MB 456 kB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: click in /usr/local/lib/python3.8/site-packages (from nltk) (7.1.2)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.8/site-packages (from nltk) (0.17.0)\n",
      "Requirement already satisfied: regex in /usr/local/lib/python3.8/site-packages (from nltk) (2020.11.13)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/site-packages (from nltk) (4.50.2)\n",
      "Building wheels for collected packages: nltk\n",
      "  Building wheel for nltk (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for nltk: filename=nltk-3.5-py3-none-any.whl size=1434674 sha256=9d81b24e719560627ee385349b9584d33cd6dafe8d869f533fc125051f0ec73c\n",
      "  Stored in directory: /Users/alditopalli/Library/Caches/pip/wheels/ff/d5/7b/f1fb4e1e1603b2f01c2424dd60fbcc50c12ef918bafc44b155\n",
      "Successfully built nltk\n",
      "Installing collected packages: nltk\n",
      "Successfully installed nltk-3.5\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'nltk.tokenizer'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-e81c9c4efbf1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtqdm\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pip3 install nltk'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizer\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTweetTokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'nltk.tokenizer'"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import sys\n",
    "import io\n",
    "import pandas as pd\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "import re\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tqdm import tqdm\n",
    "!pip3 install nltk\n",
    "from nltk.tokenizer import TweetTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(\"../data/nlp-getting-started/train.csv\", encoding=\"utf-8\")\n",
    "df_test = pd.read_csv(\"../data/nlp-getting-started/test.csv\", encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1193514it [00:23, 49938.86it/s]\n"
     ]
    }
   ],
   "source": [
    "with open(\"../data/glove.twitter.27B.50d.txt\", \"r\") as f:\n",
    "    dict_w2v = {}\n",
    "    problems = []\n",
    "    \n",
    "    for line in tqdm(f):\n",
    "        \n",
    "        tokens = line.split()\n",
    "        \n",
    "        word = tokens[0]\n",
    "        vector = np.array(tokens[1:], dtype=np.float32)\n",
    "        \n",
    "        if vector.shape[0] == 50:\n",
    "            dict_w2v[word] = vector\n",
    "        else:\n",
    "            problems.append({word: vector})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(49,)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(df):\n",
    "    #remove html tags\n",
    "    df[\"text\"] = df[\"text\"].apply(lambda x: re.sub(r'<.*?>', '', x))\n",
    "    \n",
    "    # replace urls with <url> tag\n",
    "    df[\"text\"] = df[\"text\"].apply(lambda x: re.sub(r'https?://\\S+|www\\.\\S+', '<url>', x))\n",
    "    # replace user names with <user> tag\n",
    "    df[\"text\"] = df[\"text\"].apply(lambda x: re.sub(r'@[a-zA-Z0-9_]+', '<user>', x))\n",
    "    # replace hashtags with <hashtag> tag\n",
    "    df[\"text\"] = df[\"text\"].apply(lambda x: re.sub(r'#[a-zA-Z0-9_]+', '<hashtag>', x))\n",
    "    # replace noisy words - here it can be improved\n",
    "    df[\"text\"] = df[\"text\"].apply(lambda x: x.replace(\"\\x89\", \"\").replace(\"hÛ_\", \"\").replace(\"ÛÓ\", \"\"))\n",
    "    # replace the happy emojis with <smile> tag\n",
    "    df[\"text\"] = df[\"text\"].apply(lambda x: re.sub(r'(:|;)-?(\\)|D|d)', \"<smile>\", x))\n",
    "    # replace the sad emojis with <smile> tag\n",
    "    df[\"text\"] = df[\"text\"].apply(lambda x: re.sub(r'(:|;)-?\\(+', \"<sad>\", x))\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = clean_data(df_train)\n",
    "df_test = clean_data(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "tokenizer = TweetTokenizer()\n",
    "\n",
    "tokens_train = [tokenizer.tokenize(tweet) for tweet in df_train[\"text\"]]\n",
    "tokens_test = [tokenizer.tokenize(tweet) for tweet in df_test[\"text\"]]\n",
    "\n",
    "vocab = Dictionary(tokens_train + tokens_test)\n",
    "\n",
    "special_tokens = {\"<pad>\": 0}\n",
    "vocab.patch_with_special_tokens(special_tokens)\n",
    "\n",
    "X_train = [vocab.doc2idx(token) for token in tokens_train]\n",
    "y_train = df_train[\"target\"].values\n",
    "X_test  = [vocab.doc2idx(token) for token in tokens_test]\n",
    "\n",
    "w2v_train = [[dict_w2v[token] if token in dict_w2v else dict_w2v[\"<unknown>\"] for token in list_tokens]\n",
    "             for list_tokens in tokens_train]\n",
    "w2v_test  = [[dict_w2v[token] if token in dict_w2v else dict_w2v[\"<unknown>\"] for token in list_tokens]\n",
    "             for list_tokens in tokens_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pad_sequences(X_train)\n",
    "\n",
    "w2v_train = np.array([w2_seq + [np.zeros(50)] * (X_train.shape[1] - len(w2_seq)) for w2_seq in w2v_train])\n",
    "\n",
    "X_test = pad_sequences(X_test)\n",
    "w2v_test = np.array([w2_seq + [np.zeros(50)] * (X_test.shape[1] - len(w2_seq)) for w2_seq in w2v_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_tokens = tf.keras.layers.Input(shape=X_train.shape[1], name=\"input_tokens\")\n",
    "input_w2v = tf.keras.layers.Input(shape=(X_train.shape[1], 50), name=\"input_w2v\")\n",
    "embeddings = tf.keras.layers.Embedding(len(vocab.token2id)+1, 25, mask_zero=True)(input_tokens)\n",
    "embeddings = tf.keras.layers.SpatialDropout1D(0.2)(embeddings)\n",
    "lstms = tf.keras.layers.LSTM(64, dropout=0.5, recurrent_dropout=0.2)(tf.concat((embeddings, input_w2v), axis=2))\n",
    "outputs = tf.keras.layers.Dense(1, activation=\"sigmoid\", kernel_regularizer=tf.keras.regularizers.l2(0.01))(lstms)\n",
    "\n",
    "model = tf.keras.models.Model(inputs=[input_tokens, input_w2v], outputs=outputs)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-4)\n",
    "model.compile(optimizer=optimizer, loss=\"binary_crossentropy\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "215/215 [==============================] - 18s 84ms/step - loss: 0.6966 - accuracy: 0.5691 - val_loss: 0.6584 - val_accuracy: 0.5696\n",
      "Epoch 2/15\n",
      "215/215 [==============================] - 16s 73ms/step - loss: 0.6302 - accuracy: 0.6668 - val_loss: 0.5870 - val_accuracy: 0.7165\n",
      "Epoch 3/15\n",
      "215/215 [==============================] - 16s 73ms/step - loss: 0.6016 - accuracy: 0.6945 - val_loss: 0.5728 - val_accuracy: 0.7257\n",
      "Epoch 4/15\n",
      "215/215 [==============================] - 16s 75ms/step - loss: 0.5803 - accuracy: 0.7184 - val_loss: 0.5778 - val_accuracy: 0.7152\n",
      "Epoch 5/15\n",
      "215/215 [==============================] - 16s 73ms/step - loss: 0.5645 - accuracy: 0.7282 - val_loss: 0.5726 - val_accuracy: 0.7323\n",
      "Epoch 6/15\n",
      "215/215 [==============================] - 16s 73ms/step - loss: 0.5544 - accuracy: 0.7317 - val_loss: 0.5448 - val_accuracy: 0.7375\n",
      "Epoch 7/15\n",
      "215/215 [==============================] - 16s 75ms/step - loss: 0.5319 - accuracy: 0.7501 - val_loss: 0.5444 - val_accuracy: 0.7441\n",
      "Epoch 8/15\n",
      "215/215 [==============================] - 16s 73ms/step - loss: 0.5114 - accuracy: 0.7676 - val_loss: 0.5261 - val_accuracy: 0.7572\n",
      "Epoch 9/15\n",
      "215/215 [==============================] - 16s 74ms/step - loss: 0.4878 - accuracy: 0.7884 - val_loss: 0.5265 - val_accuracy: 0.7467\n",
      "Epoch 10/15\n",
      "215/215 [==============================] - 16s 75ms/step - loss: 0.4628 - accuracy: 0.8098 - val_loss: 0.5082 - val_accuracy: 0.7664\n",
      "Epoch 11/15\n",
      "215/215 [==============================] - 16s 74ms/step - loss: 0.4329 - accuracy: 0.8263 - val_loss: 0.4959 - val_accuracy: 0.7756\n",
      "Epoch 12/15\n",
      "215/215 [==============================] - 16s 75ms/step - loss: 0.3947 - accuracy: 0.8448 - val_loss: 0.4921 - val_accuracy: 0.7900\n",
      "Epoch 13/15\n",
      "215/215 [==============================] - 17s 78ms/step - loss: 0.3669 - accuracy: 0.8625 - val_loss: 0.4881 - val_accuracy: 0.7730\n",
      "Epoch 14/15\n",
      "215/215 [==============================] - 17s 80ms/step - loss: 0.3408 - accuracy: 0.8787 - val_loss: 0.4952 - val_accuracy: 0.7769\n",
      "Epoch 15/15\n",
      "215/215 [==============================] - 16s 75ms/step - loss: 0.3233 - accuracy: 0.8831 - val_loss: 0.4907 - val_accuracy: 0.7822\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x150ecb6d0>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "early_stop_cb = tf.keras.callbacks.EarlyStopping(monitor=\"val_accuracy\", patience=5, restore_best_weights=True)\n",
    "model.fit([X_train, w2v_train], y_train, batch_size=32, epochs=15, validation_split=0.1, callbacks=[early_stop_cb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model([X_test, w2v_test])\n",
    "y_pred = [0 if y_pred_val < 0.5 else 1 for y_pred_val in y_pred]\n",
    "\n",
    "df_pred = pd.DataFrame(df_test[\"id\"])\n",
    "df_pred[\"target\"] = y_pred\n",
    "df_pred.to_csv(\"submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just happened a terrible car crash</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Heard about &lt;hashtag&gt; is different cities, sta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>there is a forest fire at spot pond, geese are...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Apocalypse lighting. &lt;hashtag&gt; &lt;hashtag&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Typhoon Soudelor kills 28 in China and Taiwan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3258</th>\n",
       "      <td>10861</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>EARTHQUAKE SAFETY LOS ANGELES ÛÒ SAFETY FASTEN...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3259</th>\n",
       "      <td>10865</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Storm in RI worse than last hurricane. My city...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3260</th>\n",
       "      <td>10868</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Green Line derailment in Chicago &lt;url&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3261</th>\n",
       "      <td>10874</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>MEG issues Hazardous Weather Outlook (HWO) &lt;url&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3262</th>\n",
       "      <td>10875</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;hashtag&gt; has activated its Municipal Emergenc...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3263 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id keyword location  \\\n",
       "0         0     NaN      NaN   \n",
       "1         2     NaN      NaN   \n",
       "2         3     NaN      NaN   \n",
       "3         9     NaN      NaN   \n",
       "4        11     NaN      NaN   \n",
       "...     ...     ...      ...   \n",
       "3258  10861     NaN      NaN   \n",
       "3259  10865     NaN      NaN   \n",
       "3260  10868     NaN      NaN   \n",
       "3261  10874     NaN      NaN   \n",
       "3262  10875     NaN      NaN   \n",
       "\n",
       "                                                   text  \n",
       "0                    Just happened a terrible car crash  \n",
       "1     Heard about <hashtag> is different cities, sta...  \n",
       "2     there is a forest fire at spot pond, geese are...  \n",
       "3              Apocalypse lighting. <hashtag> <hashtag>  \n",
       "4         Typhoon Soudelor kills 28 in China and Taiwan  \n",
       "...                                                 ...  \n",
       "3258  EARTHQUAKE SAFETY LOS ANGELES ÛÒ SAFETY FASTEN...  \n",
       "3259  Storm in RI worse than last hurricane. My city...  \n",
       "3260             Green Line derailment in Chicago <url>  \n",
       "3261   MEG issues Hazardous Weather Outlook (HWO) <url>  \n",
       "3262  <hashtag> has activated its Municipal Emergenc...  \n",
       "\n",
       "[3263 rows x 4 columns]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
