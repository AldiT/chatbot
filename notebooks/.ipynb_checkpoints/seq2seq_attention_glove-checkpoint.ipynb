{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import tensorflow_addons as tfa\n",
    "from sklearn.model_selection import train_test_split\n",
    "import time\n",
    "import os\n",
    "import sys\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What we need\n",
    "* Data\n",
    "    * Read the raw data\n",
    "    * Prepare it for processing (put it in couples (enc-dec) and clean the symbols)\n",
    "* Model\n",
    "    * Create the tokenizer\n",
    "    * Create the embedding layer\n",
    "    * Add the BiLSTM of the encoder\n",
    "    * Create the attention layer\n",
    "    * Create the Decoder (attention and LSTM?)\n",
    "* Create the inference notebook cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "oh, thanks !  i'm fine. this is an evening in my timezone\n",
      "ðŸ˜„ here is afternoon ! \n"
     ]
    }
   ],
   "source": [
    "data_path_human = \"../data/rDany/human_text.txt\"\n",
    "data_path_robot = \"../data/rDany/robot_text.txt\"\n",
    "\n",
    "with open(data_path_human, \"r\") as f:\n",
    "    human_lines = f.read().split(\"\\n\")\n",
    "    \n",
    "with open(data_path_robot, \"r\") as f:\n",
    "    robot_lines = f.read().split(\"\\n\")\n",
    "print(human_lines[1])\n",
    "print(robot_lines[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2363"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "human_lines = [re.sub(r\"\\[\\w+\\]\",'hi',line) for line in human_lines]\n",
    "human_lines = [\" \".join(re.findall(r\"\\w+\",line)) for line in human_lines]\n",
    "robot_lines = [re.sub(r\"\\[\\w+\\]\",'',line) for line in robot_lines]\n",
    "robot_lines = [\" \".join(re.findall(r\"\\w+\",line)) for line in robot_lines]\n",
    "# grouping lines by response pair\n",
    "pairs = list(zip(human_lines,robot_lines))\n",
    "#random.shuffle(pairs)\n",
    "len(pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_docs = []\n",
    "target_docs = []\n",
    "input_tokens = set()\n",
    "target_tokens = set()\n",
    "\n",
    "for line in pairs:\n",
    "    input_doc, target_doc = line[0], line[1]\n",
    "    # Appending each input sentence to input_docs\n",
    "    input_docs.append(input_doc)\n",
    "    # Splitting words from punctuation  \n",
    "    target_doc = \" \".join(re.findall(r\"[\\w']+|[^\\s\\w]\", target_doc))\n",
    "    # Redefine target_doc below and append it to target_docs\n",
    "    target_doc = '<START> ' + target_doc + ' <END>'\n",
    "    target_docs.append(target_doc)\n",
    "  \n",
    "    # Now we split up each sentence into words and add each unique word to our vocabulary set\n",
    "    for token in re.findall(r\"[\\w']+|[^\\s\\w]\", input_doc):\n",
    "        if token not in input_tokens:\n",
    "            input_tokens.add(token)\n",
    "    for token in target_doc.split():\n",
    "        if token not in target_tokens:\n",
    "            target_tokens.add(token)\n",
    "input_tokens = sorted(list(input_tokens))\n",
    "target_tokens = sorted(list(target_tokens))\n",
    "num_encoder_tokens = len(input_tokens)\n",
    "num_decoder_tokens = len(target_tokens)\n",
    "num_tokens = len(set(input_tokens + target_tokens)) + 2 # [UNK]\n",
    "pairs = list(zip(input_docs, target_docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(filters='', oov_token=\"<unk>\")\n",
    "tokenizer.fit_on_texts(input_docs + target_docs)\n",
    "\n",
    "X = tokenizer.texts_to_sequences(input_docs)\n",
    "Y = tokenizer.texts_to_sequences(target_docs)\n",
    "\n",
    "X = tf.keras.preprocessing.sequence.pad_sequences(X, padding='pre')\n",
    "Y = tf.keras.preprocessing.sequence.pad_sequences(Y, padding='pre')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1193514it [00:23, 50108.78it/s]\n"
     ]
    }
   ],
   "source": [
    "with open(\"../data/glove.twitter.27B.50d.txt\", \"r\") as f:\n",
    "    dict_w2v = {}\n",
    "    problems = []\n",
    "    \n",
    "    for line in tqdm(f):\n",
    "        \n",
    "        tokens = line.split()\n",
    "        \n",
    "        word = tokens[0]\n",
    "        vector = np.array(tokens[1:], dtype=np.float32)\n",
    "        \n",
    "        if vector.shape[0] == 50:\n",
    "            dict_w2v[word] = vector\n",
    "        else:\n",
    "            problems.append({word: vector})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(tokenizer.word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5056/5056 [00:00<00:00, 205271.57it/s]\n"
     ]
    }
   ],
   "source": [
    "num_tokens = len(tokenizer.word_index)\n",
    "embedding_dim = 50\n",
    "hits = 0\n",
    "misses = 0\n",
    "embedding_matrix = np.zeros((num_tokens, embedding_dim))\n",
    "for token, _ in tqdm(tokenizer.word_index.items()):\n",
    "    \n",
    "    embedding = dict_w2v.get(token)\n",
    "    \n",
    "    if embedding is not None:\n",
    "        embedding_matrix[tokenizer.word_index[token]] = embedding\n",
    "        hits += 1\n",
    "    else:\n",
    "        misses += 1\n",
    "embedding_matrix[tokenizer.word_index[\"<unk>\"]] = np.random.rand(embedding_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hits: 4519\n",
      "Missed: 537\n"
     ]
    }
   ],
   "source": [
    "print(f\"Hits: {hits}\")\n",
    "print(f\"Missed: {misses}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = len(X_train)\n",
    "BATCH_SIZE = 64\n",
    "steps_per_epoch = len(X_train)//BATCH_SIZE\n",
    "embedding_dim = 50\n",
    "units = 1024\n",
    "vocab_size = len(tokenizer.word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices((X_train, Y_train)).shuffle(BUFFER_SIZE).batch(BATCH_SIZE, \n",
    "                                                                                            drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_len(sentence):\n",
    "    return max(len(s) for s in sentence)\n",
    "\n",
    "max_length_input = max_len(X_train)\n",
    "max_length_output = max_len(Y_train)\n",
    "vocab_size = len(tokenizer.word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 258)\n",
      "(64, 149)\n"
     ]
    }
   ],
   "source": [
    "for example in dataset.take(1):\n",
    "    example_x, example_y = example\n",
    "    \n",
    "print(example_x.shape) \n",
    "print(example_y.shape) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderAttention(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dims, hidden_units):\n",
    "        super().__init__()\n",
    "        self.hidden_units = hidden_units\n",
    "        self.embedding_layer = tf.keras.layers.Embedding(vocab_size, embedding_dims, tf.keras.initializers.Constant(embedding_matrix),\n",
    "                trainable=True)\n",
    "        self.lstm_layer = tf.keras.layers.LSTM(hidden_units, return_sequences=True, \n",
    "                                                     return_state=True ) # We need the lstm outputs \n",
    "                                                                         # to calculate attention!\n",
    "    \n",
    "    def initialize_hidden_state(self): \n",
    "        return [tf.zeros((BATCH_SIZE, self.hidden_units)), \n",
    "                tf.zeros((BATCH_SIZE, self.hidden_units))] \n",
    "                                                               \n",
    "    def call(self, inp, hidden_state):\n",
    "        embedding = self.embedding_layer(inp)\n",
    "        output, h_state, c_state = self.lstm_layer(embedding, initial_state = hidden_state)\n",
    "        return output, h_state, c_state\n",
    "\n",
    "\n",
    "encoder = EncoderAttention(len(tokenizer.word_index), embedding_dim, units)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 258, 1024)\n",
      "(64, 1024)\n"
     ]
    }
   ],
   "source": [
    "# Test  the encoder\n",
    "sample_initial_state = encoder.initialize_hidden_state()\n",
    "sample_output, sample_h, sample_c = encoder(example_x, sample_initial_state)\n",
    "print(sample_output.shape)\n",
    "print(sample_h.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class DecoderAttention(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_units):\n",
    "        super().__init__()\n",
    "        \n",
    "        \n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim, tf.keras.initializers.Constant(embedding_matrix),\n",
    "                trainable=True)\n",
    "\n",
    "        self.lstm_cell = tf.keras.layers.LSTMCell(hidden_units)\n",
    "\n",
    "        self.sampler = tfa.seq2seq.sampler.TrainingSampler()\n",
    "\n",
    "        self.attention_mechanism = tfa.seq2seq.LuongAttention(hidden_units, memory_sequence_length=BATCH_SIZE*[len(X_train[0])]) #N\n",
    "\n",
    "        self.attention_cell = tfa.seq2seq.AttentionWrapper(cell=self.lstm_cell, # N\n",
    "                                      attention_mechanism=self.attention_mechanism, \n",
    "                                      attention_layer_size=hidden_units)\n",
    "\n",
    "        self.output_layer = tf.keras.layers.Dense(vocab_size)\n",
    "        self.decoder = tfa.seq2seq.BasicDecoder(self.attention_cell, # N\n",
    "                                                sampler=self.sampler, \n",
    "                                                output_layer=self.output_layer)\n",
    "\n",
    "    def build_initial_state(self, batch_size, encoder_state): #N\n",
    "        decoder_initial_state = self.attention_cell.get_initial_state(batch_size=batch_size, dtype=tf.float32)\n",
    "        decoder_initial_state = decoder_initial_state.clone(cell_state=encoder_state)\n",
    "        return decoder_initial_state\n",
    "\n",
    "\n",
    "    def call(self, inputs, initial_state):\n",
    "        embedding = self.embedding(inputs)\n",
    "        outputs, _, _ = self.decoder(embedding, initial_state=initial_state, sequence_length=BATCH_SIZE*[len(Y_train[0])-1])\n",
    "        return outputs\n",
    "\n",
    "decoder = DecoderAttention(len(tokenizer.word_index), embedding_dim, units)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 148, 5056)\n"
     ]
    }
   ],
   "source": [
    "# Test the decoder\n",
    "sample_y = tf.random.uniform((BATCH_SIZE, len(X_train)))\n",
    "decoder.attention_mechanism.setup_memory(sample_output) # Attention needs the last output of the Encoder\n",
    "                                                        # as starting point\n",
    "initial_state = decoder.build_initial_state(BATCH_SIZE, [sample_h, sample_c]) # N\n",
    "\n",
    "\n",
    "sample_decoder_output = decoder(example_y, initial_state)\n",
    "\n",
    "print(sample_decoder_output.rnn_output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([64, 258, 1024])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    cross_entropy = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
    "    loss = cross_entropy(y_true=real, y_pred=pred)\n",
    "    mask = tf.logical_not(tf.math.equal(real,0))   #output 0 for y=0 else output 1\n",
    "    mask = tf.cast(mask, dtype=loss.dtype)  # mask and loss have to have the same Tensor type\n",
    "    loss = mask * loss\n",
    "    loss = tf.reduce_mean(loss) # you need one loss scalar number for the mini batch\n",
    "    return loss "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Batch 0 Loss 0.5201\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 1\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    start = time.time()\n",
    "\n",
    "    encoder_hidden = encoder.initialize_hidden_state() # Every epoch we use a zero Tensor matrix\n",
    "    epoch_loss = 0\n",
    "\n",
    "    for (batch, (input, target)) in enumerate(dataset.take(steps_per_epoch)):\n",
    "        with tf.GradientTape() as tape:\n",
    "            # Pass the input through the encoder \n",
    "            encoder_output, encoder_h, encoder_c = encoder(input, encoder_hidden)\n",
    "            decoder_input = target[ : , :-1 ] # Ignore <end> token\n",
    "            real = target[ : , 1: ]         # ignore <start> token\n",
    "            # The encoder output, encoder hidden state and the decoder input\n",
    "            # is passed to the decoder\n",
    "            decoder.attention_mechanism.setup_memory(encoder_output) # N\n",
    "            decoder_initial_state = decoder.build_initial_state(BATCH_SIZE, [encoder_h, encoder_c]) # N\n",
    "            decoder_output = decoder(decoder_input, decoder_initial_state) \n",
    "            logits = decoder_output.rnn_output\n",
    "            batch_loss = loss_function(real, logits)\n",
    "\n",
    "        variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "        gradients = tape.gradient(batch_loss, variables)\n",
    "        optimizer.apply_gradients(zip(gradients, variables))\n",
    "        epoch_loss += batch_loss\n",
    "\n",
    "        if batch % 10 == 0:\n",
    "            print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1,\n",
    "                                                       batch,\n",
    "                                                       batch_loss.numpy()))\n",
    "    print('Epoch {} Loss {:.4f}'.format(epoch + 1,\n",
    "                                      epoch_loss / steps_per_epoch))\n",
    "    print('Time {:.4f} sec\\n'.format(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "def preprocess_sentence(w):\n",
    "    w = w.lower().strip()\n",
    "    # This next line is confusing!\n",
    "    # We normalize unicode data, umlauts will be converted to normal letters\n",
    "    #w = w.replace(\"ÃŸ\", \"ss\")\n",
    "    #w = ''.join(c for c in unicodedata.normalize('NFD', w) if unicodedata.category(c) != 'Mn')\n",
    "\n",
    "    # creating a space between a word and the punctuation following it\n",
    "    # eg: \"he is a boy.\" => \"he is a boy .\"\n",
    "    # Reference:- https://stackoverflow.com/questions/3645931/python-padding-punctuation-with-white-spaces-keeping-punctuation\n",
    "    w = re.sub(r\"\\[\\w+\\]\",'', w)\n",
    "    w = \" \".join(re.findall(r\"\\w+\",w))\n",
    "    w = re.sub(r\"([?.!,Â¿])\", r\" \\1 \", w)\n",
    "    w = re.sub(r'[\" \"]+', \" \", w)\n",
    "\n",
    "    # replacing everything with space except (a-z, A-Z, \".\", \"?\", \"!\", \",\")\n",
    "    w = re.sub(r\"[^a-zA-Z?.!]+\", \" \", w)\n",
    "    w = w.strip()\n",
    "\n",
    "    # adding a start and an end token to the sentence\n",
    "    # so that the model know when to start and stop predicting.\n",
    "    w = '<start> ' + w + ' <end>'\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(sentence, preprocess=True):\n",
    "    if preprocess:\n",
    "        sentence = preprocess_sentence(sentence)\n",
    "        sentence_tokens = tokenizer.texts_to_sequences([sentence])\n",
    "        input = tf.keras.preprocessing.sequence.pad_sequences(sentence_tokens, maxlen=max_length_input, padding='post')\n",
    "    else:\n",
    "        input = sentence\n",
    "    input = tf.convert_to_tensor(input)\n",
    "\n",
    "    encoder_hidden = [tf.zeros((1, units)), tf.zeros((1, units))]\n",
    "    encoder_output, encoder_h, encoder_c = encoder(input, encoder_hidden)\n",
    "    start_token = tf.convert_to_tensor([tokenizer.word_index['<start>']])\n",
    "    end_token = tokenizer.word_index['<end>']\n",
    "\n",
    "    # This time we use the greedy sampler because we want the word with the highest probability!\n",
    "    # We are not generating new text, where a probability sampling would be better\n",
    "    greedy_sampler = tfa.seq2seq.GreedyEmbeddingSampler()\n",
    "\n",
    "    # Instantiate a BasicDecoder object\n",
    "    decoder_instance = tfa.seq2seq.BasicDecoder(cell=decoder.attention_cell, # N\n",
    "                                                sampler=greedy_sampler, output_layer=decoder.output_layer)\n",
    "    # Setup Memory in decoder stack\n",
    "    decoder.attention_mechanism.setup_memory(encoder_output) # N\n",
    "\n",
    "    # set decoder_initial_state\n",
    "    decoder_initial_state = decoder.build_initial_state(batch_size=1, encoder_state=[encoder_h, encoder_c]) # N\n",
    "\n",
    "    ### Since the BasicDecoder wraps around Decoder's rnn cell only, you have to ensure that the inputs to BasicDecoder \n",
    "    ### decoding step is output of embedding layer. tfa.seq2seq.GreedyEmbeddingSampler() takes care of this. \n",
    "    ### You only need to get the weights of embedding layer, which can be done by decoder.embedding.variables[0] and pass this callabble to BasicDecoder's call() function\n",
    "\n",
    "    decoder_embedding_matrix = decoder.embedding.variables[0]\n",
    "\n",
    "    outputs, _, _ = decoder_instance(decoder_embedding_matrix, start_tokens = start_token, end_token= end_token, initial_state=decoder_initial_state)\n",
    "\n",
    "    result_sequence  = outputs.sample_id.numpy()\n",
    "    return tokenizer.sequences_to_texts(result_sequence)[0]\n",
    "\n",
    "translate(\"I love you!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "tokenizer = Tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(text_vectorization_layer.get_vocabulary()[:12])\n",
    "text_vectorization_layer([\"<START>\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder.embedding.variables[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
