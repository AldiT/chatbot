{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "seq2seq_with_attention_glove_50d.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "gYYarrjnZLbE"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive/\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gCKovzf-ZbLu"
      },
      "source": [
        "!ls /content/drive/MyDrive/seminar/seminar/rDany\n",
        "!pip install spacy_langdetect"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xl3J1fhNaOLg"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "import tensorflow_addons as tfa\n",
        "from sklearn.model_selection import train_test_split\n",
        "from bs4 import BeautifulSoup\n",
        "import pickle as pkl\n",
        "import unicodedata\n",
        "import spacy\n",
        "from spacy_langdetect import LanguageDetector\n",
        "import time\n",
        "import os\n",
        "import sys\n",
        "import re"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y0-9bRBeg5Gb"
      },
      "source": [
        "human_lines = []\n",
        "robot_lines = []"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "__-Tzm_MabXk"
      },
      "source": [
        "data_path_human = \"/content/drive/MyDrive/seminar/seminar/rDany/human_text.txt\"\n",
        "data_path_robot = \"/content/drive/MyDrive/seminar/seminar/rDany/robot_text.txt\"\n",
        "\n",
        "\n",
        "with open(data_path_human, \"r\") as f:\n",
        "    human_lines += f.read().split(\"\\n\")\n",
        "    \n",
        "with open(data_path_robot, \"r\") as f:\n",
        "    robot_lines += f.read().split(\"\\n\")\n",
        "print(human_lines[1])\n",
        "print(robot_lines[1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xoltnv6_eKgV"
      },
      "source": [
        "with open(\"/content/drive/MyDrive/seminar/seminar/input_docs.pkl\", \"rb\") as handle:\n",
        "  human_lines += pkl.load(handle)\n",
        "with open(\"/content/drive/MyDrive/seminar/seminar/target_docs.pkl\", \"rb\") as handle:\n",
        "  robot_lines += pkl.load(handle)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YuecJHi6QN_r"
      },
      "source": [
        "\n",
        "# function to remove accented characters\n",
        "def remove_accented_chars(text):\n",
        "    new_text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
        "    return new_text\n",
        "\n",
        "for i, line in tqdm(enumerate(human_lines)):\n",
        "  human_lines[i] = remove_accented_chars(line)\n",
        "\n",
        "for i, line in tqdm(enumerate(robot_lines)):\n",
        "  robot_lines[i] = remove_accented_chars(line) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IofOA9zbT6Vg"
      },
      "source": [
        "english_human_lines = []\n",
        "english_robot_lines = []\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BXuDtXRuTLRJ"
      },
      "source": [
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "nlp.add_pipe(LanguageDetector(), name=\"language_detector\", last=True)\n",
        "\n",
        "for i in tqdm(range(len(human_lines))):\n",
        "  human_doc = nlp(human_lines[i])\n",
        "  robot_doc = nlp(robot_lines[i])\n",
        "\n",
        "  if human_doc._.language['language'] == 'en' and robot_doc._.language['language'] == 'en':\n",
        "    english_human_lines.append(human_lines[i])\n",
        "    english_robot_lines.append(robot_lines[i])\n",
        "print(len(english_human_lines), len(english_robot_lines))\n",
        "human_lines = english_human_lines\n",
        "robot_lines = english_robot_lines\n",
        "del english_human_lines\n",
        "del english_robot_lines"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QGWdz-5Hd9BO"
      },
      "source": [
        "#with open(\"/content/drive/MyDrive/seminar/english_human.pkl\", \"wb\") as handle:\n",
        "#  pkl.dump(human_lines, handle)\n",
        "\n",
        "#with open(\"/content/drive/MyDrive/seminar/english_robot.pkl\", \"wb\") as handle:\n",
        "#  pkl.dump(robot_lines, handle)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hljlE0xvgB99"
      },
      "source": [
        "!ls /content/drive/MyDrive/seminar\n",
        "!mv /content/drive/MyDrive/seminar/english_human\\ \\(1\\).pkl /content/drive/MyDrive/seminar/english_human.pkl\n",
        "!mv /content/drive/MyDrive/seminar/english_robot\\ \\(1\\).pkl /content/drive/MyDrive/seminar/english_robot.pkl"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FM2ZG2IaFV1Y"
      },
      "source": [
        "with open(\"/content/drive/MyDrive/seminar/seminar/english_human.pkl\", \"rb\") as handle:\n",
        "  human_lines = pkl.load(handle)\n",
        "\n",
        "with open(\"/content/drive/MyDrive/seminar/seminar/english_robot.pkl\", \"rb\") as handle:\n",
        "  robot_lines = pkl.load(handle)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qHaawQs9ugtP"
      },
      "source": [
        "human_lines[3002], robot_lines[3002]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "llAsktlLNF5o"
      },
      "source": [
        "import re\n",
        "\n",
        "def decontracted(phrase):\n",
        "    # specific\n",
        "    phrase = re.sub(r\"won\\'t\", \"will not\", phrase)\n",
        "    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n",
        "\n",
        "    # general\n",
        "    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n",
        "    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n",
        "    phrase = re.sub(r\"\\'s\", \" is\", phrase)\n",
        "    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n",
        "    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n",
        "    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n",
        "    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n",
        "    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n",
        "    return phrase\n",
        "\n",
        "\n",
        "test = \"Hey I'm Yann, how're you and how's it going ? That's interesting: I'd love to hear more about it.\"\n",
        "print(decontracted(test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HEBiUKBLIpVA"
      },
      "source": [
        "\n",
        "for i, line in tqdm(enumerate(human_lines)):\n",
        "  human_lines[i] = decontracted(line)\n",
        "\n",
        "for i, line in tqdm(enumerate(robot_lines)):\n",
        "  robot_lines[i] = decontracted(line)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PlWd7PPGNl4F"
      },
      "source": [
        "# Spell Check"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gBWTstoJaj2H"
      },
      "source": [
        "human_lines = [re.sub(r\"\\[\\w+\\]\",'hi',line) for line in human_lines]\n",
        "human_lines = [\" \".join(re.findall(r\"\\w+\",line)) for line in human_lines]\n",
        "robot_lines = [re.sub(r\"\\[\\w+\\]\",'',line) for line in robot_lines]\n",
        "robot_lines = [\" \".join(re.findall(r\"\\w+\",line)) for line in robot_lines]\n",
        "# grouping lines by response pair\n",
        "pairs = list(zip(human_lines,robot_lines))\n",
        "#random.shuffle(pairs)\n",
        "len(pairs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lXu7yNobamhA"
      },
      "source": [
        "input_docs = []\n",
        "target_docs = []\n",
        "input_tokens = set()\n",
        "target_tokens = set()\n",
        "\n",
        "for line in pairs:\n",
        "    input_doc, target_doc = line[0], line[1]\n",
        "    # Appending each input sentence to input_docs\n",
        "    input_docs.append(input_doc)\n",
        "    # Splitting words from punctuation  \n",
        "    target_doc = \" \".join(re.findall(r\"[\\w']+|[^\\s\\w]\", target_doc))\n",
        "    # Redefine target_doc below and append it to target_docs\n",
        "    target_doc = '<START> ' + target_doc + ' <END>'\n",
        "    target_docs.append(target_doc)\n",
        "  \n",
        "    # Now we split up each sentence into words and add each unique word to our vocabulary set\n",
        "    for token in re.findall(r\"[\\w']+|[^\\s\\w]\", input_doc):\n",
        "        if token not in input_tokens:\n",
        "            input_tokens.add(token)\n",
        "    for token in target_doc.split():\n",
        "        if token not in target_tokens:\n",
        "            target_tokens.add(token)\n",
        "input_tokens = sorted(list(input_tokens))\n",
        "target_tokens = sorted(list(target_tokens))\n",
        "num_encoder_tokens = len(input_tokens)\n",
        "num_decoder_tokens = len(target_tokens)\n",
        "num_tokens = len(set(input_tokens + target_tokens)) + 2 # [UNK]\n",
        "pairs = list(zip(input_docs, target_docs))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kfI0pubtsoPp"
      },
      "source": [
        "pairs[3002]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8IGS80-okUYi"
      },
      "source": [
        "vocab_size = 30000 + 1\n",
        "units = 1024\n",
        "embedding_dim = 100"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wIwgAcz5zCHA"
      },
      "source": [
        "\n",
        "\n",
        "tokenizer = Tokenizer(filters='', oov_token=\"<unk>\")\n",
        "tokenizer.fit_on_texts(input_docs + target_docs)\n",
        "\n",
        "tokenizer.num_words = vocab_size\n",
        "input_docs_tokenized = tokenizer.texts_to_sequences(input_docs)\n",
        "target_docs_tokenized = tokenizer.texts_to_sequences(target_docs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w5N3AfaWyMTP"
      },
      "source": [
        "final_in_docs_tokenized = []\n",
        "final_tar_docs_tokenized = []\n",
        "\n",
        "for i in range(len(input_docs_tokenized)):\n",
        "  if len(input_docs_tokenized[i]) <= 15 and len(target_docs_tokenized[i]) <= 15:\n",
        "    final_in_docs_tokenized.append(input_docs_tokenized[i])\n",
        "    final_tar_docs_tokenized.append(target_docs_tokenized[i])\n",
        "len(final_in_docs_tokenized), len(final_tar_docs_tokenized)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qRzIoZUR2K2p"
      },
      "source": [
        "input_docs_tokenized[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y7VMXQRYB8Bi"
      },
      "source": [
        "max_len = 0\n",
        "for r in final_tar_docs_tokenized:\n",
        "  if len(r) > max_len:\n",
        "    max_len = len(r)\n",
        "  \n",
        "max_len"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zzSow6sfa0Wi"
      },
      "source": [
        "\n",
        "X = final_in_docs_tokenized\n",
        "Y = final_tar_docs_tokenized\n",
        "\n",
        "X = tf.keras.preprocessing.sequence.pad_sequences(X, padding='pre')\n",
        "Y = tf.keras.preprocessing.sequence.pad_sequences(Y, padding='pre')\n",
        "\n",
        "del final_in_docs_tokenized\n",
        "del final_tar_docs_tokenized\n",
        "del input_docs\n",
        "del target_docs\n",
        "del input_docs_tokenized\n",
        "del target_docs_tokenized\n",
        "#del human_lines\n",
        "#del robot_lines"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wcu_SuRC0QcB"
      },
      "source": [
        "X.shape, Y.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gg8uuJuna3O6"
      },
      "source": [
        "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JXx3sLHpemYQ"
      },
      "source": [
        "print(X_train.shape, X_test.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lVFgTLh1jIQs"
      },
      "source": [
        "BUFFER_SIZE = len(X_train)\n",
        "BATCH_SIZE = 64\n",
        "steps_per_epoch = len(X_train)//BATCH_SIZE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jPZT85Nla5Vs"
      },
      "source": [
        "with open(\"/content/drive/MyDrive/seminar/seminar/enwiki_20180420_100d.txt\", \"r\") as f:\n",
        "    dict_w2v = {}\n",
        "    problems = []\n",
        "    \n",
        "    for line in tqdm(f):\n",
        "        \n",
        "        tokens = line.split()\n",
        "\n",
        "        try:\n",
        "          word = tokens[0]\n",
        "          vector = np.array(tokens[1:], dtype=np.float32)\n",
        "        except:\n",
        "          pass\n",
        "        \n",
        "        if vector.shape[0] == embedding_dim:\n",
        "            dict_w2v[word] = vector\n",
        "        else:\n",
        "            problems.append({word: vector})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ge7pv-2zVSOp"
      },
      "source": [
        "len(problems)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TdfutjjwDHPs"
      },
      "source": [
        "tokenizer.word_index[list(tokenizer.word_index.keys())[0]]\n",
        "len(dict_w2v)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jzj2kjQUa7AT"
      },
      "source": [
        "\n",
        "\n",
        "hits = 0\n",
        "misses = 0\n",
        "embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
        "tokens = list(tokenizer.word_index.keys())[:vocab_size-1]\n",
        "\n",
        "for token in tqdm(tokens):\n",
        "    \n",
        "    embedding = dict_w2v.get(token)\n",
        "    \n",
        "    if embedding is not None:\n",
        "        embedding_matrix[tokenizer.word_index[token]] = embedding\n",
        "        hits += 1\n",
        "    else:\n",
        "        misses += 1\n",
        "embedding_matrix[tokenizer.word_index[\"<unk>\"]] = np.random.rand(embedding_dim)\n",
        "print(hits, misses)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l1GEGzrbLnZz"
      },
      "source": [
        "import sys\n",
        "print(sys.getsizeof(dict_w2v))\n",
        "del dict_w2v"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eONR0Tv9cJn-"
      },
      "source": [
        "print(f\"Hits: {hits}\")\n",
        "print(f\"Missed: {misses}\")\n",
        "len(tokenizer.word_index)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GUPb9vaVgWLt"
      },
      "source": [
        "len(tokenizer.word_index)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gIhaTC9bcM_F"
      },
      "source": [
        "dataset = tf.data.Dataset.from_tensor_slices((X_train, Y_train)).shuffle(BUFFER_SIZE).batch(BATCH_SIZE, \n",
        "                                                                                            drop_remainder=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IPB6XPEkcO5F"
      },
      "source": [
        "def max_len(sentence):\n",
        "    return max(len(s) for s in sentence)\n",
        "\n",
        "max_length_input = max_len(X_train)\n",
        "max_length_output = max_len(Y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4cumw_RZcSHg"
      },
      "source": [
        "for example in dataset.take(1):\n",
        "    example_x, example_y = example\n",
        "    \n",
        "print(example_x.shape) \n",
        "print(example_y.shape) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6b3lWbbrZghu"
      },
      "source": [
        "type(example_x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oCygKUvfcUCm"
      },
      "source": [
        "class EncoderAttention(tf.keras.Model):\n",
        "    def __init__(self, vocab_size, embedding_dims, hidden_units):\n",
        "        super().__init__()\n",
        "        self.hidden_units = hidden_units\n",
        "        self.embedding_layer = tf.keras.layers.Embedding(vocab_size, embedding_dims, tf.keras.initializers.Constant(embedding_matrix),\n",
        "                trainable=True)\n",
        "        self.lstm_layer = tf.keras.layers.LSTM(hidden_units, return_sequences=True, \n",
        "                                                     return_state=True ) # We need the lstm outputs \n",
        "                                                                         # to calculate attention!\n",
        "    \n",
        "    def initialize_hidden_state(self): \n",
        "        return [tf.zeros((BATCH_SIZE, self.hidden_units)), \n",
        "                tf.zeros((BATCH_SIZE, self.hidden_units))] \n",
        "                                                               \n",
        "    def call(self, inputs, hidden_state):\n",
        "        embedding = self.embedding_layer(inputs)\n",
        "        output, h_state, c_state = self.lstm_layer(embedding, initial_state = hidden_state)\n",
        "        return output, h_state, c_state\n",
        "\n",
        "\n",
        "encoder = EncoderAttention(vocab_size, embedding_dim, units)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XC_34Va_cV-u"
      },
      "source": [
        "# Test  the encoder\n",
        "sample_initial_state = encoder.initialize_hidden_state()\n",
        "sample_output, sample_h, sample_c = encoder(example_x, sample_initial_state)\n",
        "print(sample_output.shape)\n",
        "print(sample_h.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_c17LmpfcXsG"
      },
      "source": [
        "\n",
        "class DecoderAttention(tf.keras.Model):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_units):\n",
        "        super().__init__()\n",
        "        \n",
        "        \n",
        "        self.embedding_layer = tf.keras.layers.Embedding(vocab_size, embedding_dim, tf.keras.initializers.Constant(embedding_matrix),\n",
        "                trainable=True)\n",
        "\n",
        "        self.lstm_cell = tf.keras.layers.LSTMCell(hidden_units)\n",
        "\n",
        "        self.sampler = tfa.seq2seq.sampler.TrainingSampler()\n",
        "\n",
        "        self.attention_mechanism = tfa.seq2seq.LuongAttention(hidden_units, memory_sequence_length=BATCH_SIZE*[len(X_train[0])]) #N\n",
        "\n",
        "        self.attention_cell = tfa.seq2seq.AttentionWrapper(cell=self.lstm_cell, # N\n",
        "                                      attention_mechanism=self.attention_mechanism, \n",
        "                                      attention_layer_size=hidden_units)\n",
        "\n",
        "        self.output_layer = tf.keras.layers.Dense(vocab_size)\n",
        "        self.decoder = tfa.seq2seq.BasicDecoder(self.attention_cell, # N\n",
        "                                                sampler=self.sampler, \n",
        "                                                output_layer=self.output_layer)\n",
        "\n",
        "    def build_initial_state(self, batch_size, encoder_state): #N\n",
        "        decoder_initial_state = self.attention_cell.get_initial_state(batch_size=batch_size, dtype=tf.float32)\n",
        "        decoder_initial_state = decoder_initial_state.clone(cell_state=encoder_state)\n",
        "        return decoder_initial_state\n",
        "\n",
        "\n",
        "    def call(self, inputs, initial_state):\n",
        "        embedding = self.embedding_layer(inputs)\n",
        "        outputs, _, _ = self.decoder(embedding, initial_state=initial_state, sequence_length=BATCH_SIZE*[len(Y_train[0])-1])\n",
        "        return outputs\n",
        "\n",
        "decoder = DecoderAttention(vocab_size, embedding_dim, units)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cauy3GA7cZ_F"
      },
      "source": [
        "# Test the decoder\n",
        "sample_y = tf.random.uniform((BATCH_SIZE, len(X_train)))\n",
        "decoder.attention_mechanism.setup_memory(sample_output) # Attention needs the last output of the Encoder\n",
        "                                                        # as starting point\n",
        "initial_state = decoder.build_initial_state(BATCH_SIZE, [sample_h, sample_c]) # N\n",
        "\n",
        "\n",
        "sample_decoder_output = decoder(example_y, initial_state)\n",
        "\n",
        "print(sample_decoder_output.rnn_output.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3R-CYQieccKA"
      },
      "source": [
        "sample_output.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NiIhOi2AceGG"
      },
      "source": [
        "optimizer = tf.keras.optimizers.Adam()\n",
        "\n",
        "def loss_function(real, pred):\n",
        "    cross_entropy = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
        "    loss = cross_entropy(y_true=real, y_pred=pred)\n",
        "    mask = tf.logical_not(tf.math.equal(real,0))   #output 0 for y=0 else output 1\n",
        "    mask = tf.cast(mask, dtype=loss.dtype)  # mask and loss have to have the same Tensor type\n",
        "    loss = mask * loss\n",
        "    loss = tf.reduce_mean(loss) # you need one loss scalar number for the mini batch\n",
        "    return loss "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J3GoxLDgcuPq"
      },
      "source": [
        "tf.test.is_gpu_available(cuda_only=False, min_cuda_compute_capability=None)\n",
        "import time"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bn0oqbEKcfxu"
      },
      "source": [
        "EPOCHS = 10\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    start = time.time()\n",
        "\n",
        "    encoder_hidden = encoder.initialize_hidden_state() # Every epoch we use a zero Tensor matrix\n",
        "    epoch_loss = 0\n",
        "\n",
        "    for (batch, (input, target)) in enumerate(dataset.take(steps_per_epoch)):\n",
        "        with tf.GradientTape() as tape:\n",
        "            # Pass the input through the encoder \n",
        "            encoder_output, encoder_h, encoder_c = encoder(input, encoder_hidden)\n",
        "            decoder_input = target[ : , :-1 ] # Ignore <end> token\n",
        "            real = target[ : , 1: ]         # ignore <start> token\n",
        "            # The encoder output, encoder hidden state and the decoder input\n",
        "            # is passed to the decoder\n",
        "            decoder.attention_mechanism.setup_memory(encoder_output) # N\n",
        "            decoder_initial_state = decoder.build_initial_state(BATCH_SIZE, [encoder_h, encoder_c]) # N\n",
        "            decoder_output = decoder(decoder_input, decoder_initial_state) \n",
        "            logits = decoder_output.rnn_output\n",
        "            batch_loss = loss_function(real, logits)\n",
        "\n",
        "        variables = encoder.trainable_variables + decoder.trainable_variables\n",
        "        gradients = tape.gradient(batch_loss, variables)\n",
        "        optimizer.apply_gradients(zip(gradients, variables))\n",
        "        epoch_loss += batch_loss\n",
        "\n",
        "        if batch % 100 == 0:\n",
        "            print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1,\n",
        "                                                       batch,\n",
        "                                                       batch_loss.numpy()))\n",
        "    print('Epoch {} Loss {:.4f}'.format(epoch + 1,\n",
        "                                      epoch_loss / steps_per_epoch))\n",
        "    print('Time {:.4f} sec\\n'.format(time.time() - start))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V6DCbrldkHv2"
      },
      "source": [
        "encoder_model_save_path = \"/content/drive/MyDrive/seminar/seminar/models/full_models/model_2_v3/encoder/weights.ckpt\"\n",
        "decoder_model_save_path = \"/content/drive/MyDrive/seminar/seminar/models/full_models/model_2_v3/decoder/weights.ckpt\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OJzSYHaFOJGs"
      },
      "source": [
        "with open(\"/content/drive/MyDrive/seminar/seminar/models/full_models/model_2_v3/tokenizer.pkl\", \"wb\") as handle:\n",
        "  pkl.dump(tokenizer, handle, protocol=pkl.HIGHEST_PROTOCOL)\n",
        "\n",
        "with open(\"/content/drive/MyDrive/seminar/seminar/models/full_models/model_2_v3/encoder_embedding_layer.pkl\", \"wb\") as handle:\n",
        "  pkl.dump(tf.convert_to_tensor(encoder.embedding_layer.variables).numpy(), handle, protocol=pkl.HIGHEST_PROTOCOL)\n",
        "\n",
        "with open(\"/content/drive/MyDrive/seminar/seminar/models/full_models/model_2_v3/decoder_embedding_layer.pkl\", \"wb\") as handle:\n",
        "  pkl.dump(tf.convert_to_tensor(decoder.embedding_layer.variables).numpy(), handle, protocol=pkl.HIGHEST_PROTOCOL)\n",
        "\n",
        "with open(\"/content/drive/MyDrive/seminar/seminar/models/full_models/model_2_v3/hyper_params.yaml\", \"w\") as handle:\n",
        "  hyper_params = \"\"\"\n",
        "    epochs: 70\n",
        "    batch-size: 64\n",
        "    optimizer: adam\n",
        "    vocab-size: 30000\n",
        "  \"\"\"\n",
        "  handle.write(hyper_params)\n",
        "\n",
        "encoder.save_weights(encoder_model_save_path)\n",
        "decoder.save_weights(decoder_model_save_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "se6XShmMXNgT"
      },
      "source": [
        "tf.compat.v1.enable_eager_execution()\n",
        "tf.convert_to_tensor(encoder.embedding_layer.variables).numpy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "erz4xigFCNk0"
      },
      "source": [
        "#encoder.save_weights(encoder_model_save_path)\n",
        "#decoder.save_weights(decoder_model_save_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d9MYUTq-Bquy"
      },
      "source": [
        "\n",
        "#encoder.save(encoder_model_save_path, save_format=\"tf\")\n",
        "#decoder.save(decoder_model_save_path, save_format=\"tf\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0IQ5u0wFkE-6"
      },
      "source": [
        "#encoder = tf.keras.models.load_model(encoder_model_save_path)\n",
        "#decoder = tf.keras.models.load_model(decoder_checkpoint_path)\n",
        "encoder.load_weights(encoder_model_save_path)\n",
        "decoder.load_weights(decoder_model_save_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MtcwvktHfKQ7"
      },
      "source": [
        "import unicodedata\n",
        "def preprocess_sentence(w):\n",
        "    w = w.lower().strip()\n",
        "    # This next line is confusing!\n",
        "    # We normalize unicode data, umlauts will be converted to normal letters\n",
        "    #w = w.replace(\"ß\", \"ss\")\n",
        "    #w = ''.join(c for c in unicodedata.normalize('NFD', w) if unicodedata.category(c) != 'Mn')\n",
        "\n",
        "    # creating a space between a word and the punctuation following it\n",
        "    # eg: \"he is a boy.\" => \"he is a boy .\"\n",
        "    # Reference:- https://stackoverflow.com/questions/3645931/python-padding-punctuation-with-white-spaces-keeping-punctuation\n",
        "    w = re.sub(r\"\\[\\w+\\]\",'', w)\n",
        "    w = \" \".join(re.findall(r\"\\w+\",w))\n",
        "    w = re.sub(r\"([?.!,¿])\", r\" \\1 \", w)\n",
        "    w = re.sub(r'[\" \"]+', \" \", w)\n",
        "\n",
        "    # replacing everything with space except (a-z, A-Z, \".\", \"?\", \"!\", \",\")\n",
        "    w = re.sub(r\"[^a-zA-Z?.!]+\", \" \", w)\n",
        "    w = w.strip()\n",
        "    w = decontracted(w)\n",
        "\n",
        "    # adding a start and an end token to the sentence\n",
        "    # so that the model know when to start and stop predicting.\n",
        "    w = '<start> ' + w + ' <end>'\n",
        "    return w"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8DO2WQ0YfLog"
      },
      "source": [
        "#def reply(sentence, preprocess=True):\n",
        "sentence = \"Hi\"\n",
        "preprocess = True\n",
        "\n",
        "if preprocess:\n",
        "    sentence = preprocess_sentence(sentence)\n",
        "    sentence_tokens = tokenizer.texts_to_sequences([sentence])\n",
        "    input = tf.keras.preprocessing.sequence.pad_sequences(sentence_tokens, maxlen=max_length_input, padding='post')\n",
        "else:\n",
        "    input = sentence\n",
        "input = tf.convert_to_tensor(input)\n",
        "\n",
        "encoder_hidden = [tf.zeros((1, units)), tf.zeros((1, units))]\n",
        "encoder_output, encoder_h, encoder_c = encoder(input, encoder_hidden)\n",
        "start_token = tf.convert_to_tensor([tokenizer.word_index['<start>']])\n",
        "end_token = tokenizer.word_index['<end>']\n",
        "\n",
        "# This time we use the greedy sampler because we want the word with the highest probability!\n",
        "# We are not generating new text, where a probability sampling would be better\n",
        "greedy_sampler = tfa.seq2seq.GreedyEmbeddingSampler()\n",
        "\n",
        "# Instantiate a BasicDecoder object\n",
        "decoder_instance = tfa.seq2seq.BasicDecoder(cell=decoder.attention_cell, # N\n",
        "                                            sampler=greedy_sampler, output_layer=decoder.output_layer)\n",
        "# Setup Memory in decoder stack\n",
        "decoder.attention_mechanism.setup_memory(encoder_output) # N\n",
        "\n",
        "# set decoder_initial_state\n",
        "decoder_initial_state = decoder.build_initial_state(batch_size=1, encoder_state=[encoder_h, encoder_c]) # N\n",
        "\n",
        "### Since the BasicDecoder wraps around Decoder's rnn cell only, you have to ensure that the inputs to BasicDecoder \n",
        "### decoding step is output of embedding layer. tfa.seq2seq.GreedyEmbeddingSampler() takes care of this. \n",
        "### You only need to get the weights of embedding layer, which can be done by decoder.embedding.variables[0] and pass this callabble to BasicDecoder's call() function\n",
        "\n",
        "decoder_embedding_matrix = decoder.embedding_layer.variables[0]\n",
        "\n",
        "outputs, _, _ = decoder_instance(decoder_embedding_matrix, start_tokens = start_token, end_token= end_token, initial_state=decoder_initial_state)\n",
        "\n",
        "result_sequence  = outputs.sample_id.numpy()\n",
        "#return tokenizer.sequences_to_texts(result_sequence)[0]\n",
        "print(tokenizer.sequences_to_texts(result_sequence)[0])\n",
        "#reply(\"Hi\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "10tAZ4YLslyQ"
      },
      "source": [
        "tfa.__version__"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AGMcKXC-furO"
      },
      "source": [
        "reply(\"How about we go hiking?\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ATXX7zQvfwDE"
      },
      "source": [
        "decoder.embedding_layer.variables"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lUM48EiefyTd"
      },
      "source": [
        "encoder.load_weights(encoder_checkpoint_path)\n",
        "decoder.load_weights(decoder_checkpoint_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YmxlA_HEBnrm"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}