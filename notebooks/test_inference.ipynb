{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import pickle as pkl\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_path_encoder = \"../chatbot/model_v1/model_2_v3/encoder/weights.ckpt\"\n",
    "weight_path_decoder = \"../chatbot/model_v1/model_2_v3/encoder/weights.ckpt\"\n",
    "tokenizer_path = \"../chatbot/model_v1/model_2_v3/tokenizer.pkl\"\n",
    "encoder_embedding_layer_path = \"../chatbot/model_v1/model_2_v3/encoder_embedding_layer.pkl\"\n",
    "decoder_embedding_layer_path = \"../chatbot/model_v1/model_2_v3/decoder_embedding_layer.pkl\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 30000 + 1\n",
    "units = 1024\n",
    "embedding_dim = 100\n",
    "BATCH_SIZE=64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open(tokenizer_path, \"rb\") as handle:\n",
    "    tokenizer = pkl.load(handle)\n",
    "\n",
    "with open(encoder_embedding_layer_path, \"rb\") as handle:\n",
    "    encoder_embedding_variables = pkl.load(handle)\n",
    "\n",
    "with open(decoder_embedding_layer_path, \"rb\") as handle:\n",
    "    decoder_embedding_variables = pkl.load(handle)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderAttention(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dims, hidden_units):\n",
    "        super().__init__()\n",
    "        self.hidden_units = hidden_units\n",
    "        self.embedding_layer = tf.keras.layers.Embedding(vocab_size, embedding_dims, tf.keras.initializers.Constant(encoder_embedding_variables),\n",
    "                trainable=True)\n",
    "        self.lstm_layer = tf.keras.layers.LSTM(hidden_units, return_sequences=True, \n",
    "                                                     return_state=True ) # We need the lstm outputs \n",
    "                                                                         # to calculate attention!\n",
    "    \n",
    "    def initialize_hidden_state(self): \n",
    "        return [tf.zeros((BATCH_SIZE, self.hidden_units)), \n",
    "                tf.zeros((BATCH_SIZE, self.hidden_units))] \n",
    "                                                               \n",
    "    def call(self, inputs, hidden_state):\n",
    "        embedding = self.embedding_layer(inputs)\n",
    "        output, h_state, c_state = self.lstm_layer(embedding, initial_state = hidden_state)\n",
    "        return output, h_state, c_state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class DecoderAttention(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_units):\n",
    "        super().__init__()\n",
    "        \n",
    "        \n",
    "        self.embedding_layer = self.embedding_layer = tf.keras.layers.Embedding(vocab_size, embedding_dim, tf.keras.initializers.Constant(decoder_embedding_variables),\n",
    "                trainable=True)\n",
    "\n",
    "        self.lstm_cell = tf.keras.layers.LSTMCell(hidden_units)\n",
    "\n",
    "        self.sampler = tfa.seq2seq.sampler.TrainingSampler()\n",
    "\n",
    "        self.attention_mechanism = tfa.seq2seq.LuongAttention(hidden_units, memory_sequence_length=BATCH_SIZE*[15]) #N\n",
    "\n",
    "        self.attention_cell = tfa.seq2seq.AttentionWrapper(cell=self.lstm_cell, # N\n",
    "                                      attention_mechanism=self.attention_mechanism, \n",
    "                                      attention_layer_size=hidden_units)\n",
    "\n",
    "        self.output_layer = tf.keras.layers.Dense(vocab_size)\n",
    "        self.decoder = tfa.seq2seq.BasicDecoder(self.attention_cell, # N\n",
    "                                                sampler=self.sampler, \n",
    "                                                output_layer=self.output_layer)\n",
    "\n",
    "    def build_initial_state(self, batch_size, encoder_state): #N\n",
    "        decoder_initial_state = self.attention_cell.get_initial_state(batch_size=batch_size, dtype=tf.float32)\n",
    "        decoder_initial_state = decoder_initial_state.clone(cell_state=encoder_state)\n",
    "        return decoder_initial_state\n",
    "\n",
    "\n",
    "    def call(self, inputs, initial_state):\n",
    "        embedding = self.embedding_layer(inputs)\n",
    "        outputs, _, _ = self.decoder(embedding, initial_state=initial_state, sequence_length=BATCH_SIZE*[15-1])\n",
    "        return outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 15, 1024)\n",
      "(64, 1024)\n",
      "All set\n"
     ]
    }
   ],
   "source": [
    "example_x, example_y = tf.random.uniform((BATCH_SIZE, 15)), tf.random.uniform((BATCH_SIZE, 15))\n",
    "\n",
    "##ENCODER\n",
    "encoder = EncoderAttention(vocab_size, embedding_dim, units)\n",
    "# Test  the encoder\n",
    "sample_initial_state = encoder.initialize_hidden_state()\n",
    "sample_output, sample_h, sample_c = encoder(example_x, sample_initial_state)\n",
    "print(sample_output.shape)\n",
    "print(sample_h.shape)\n",
    "\n",
    "\n",
    "##DECODER\n",
    "decoder = DecoderAttention(vocab_size, embedding_dim, units)\n",
    "decoder.attention_mechanism.setup_memory(sample_output) # Attention needs the last output of the Encoder as starting point\n",
    "initial_state = decoder.build_initial_state(BATCH_SIZE, [sample_h, sample_c]) # N\n",
    "sample_decoder_output = decoder(example_y, initial_state)\n",
    "\n",
    "\n",
    "encoder.load_weights(weight_path_encoder)\n",
    "decoder.load_weights(weight_path_decoder)\n",
    "\n",
    "print(\"All set\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decontracted(phrase):\n",
    "    # specific\n",
    "    phrase = re.sub(r\"won\\'t\", \"will not\", phrase)\n",
    "    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n",
    "\n",
    "    # general\n",
    "    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n",
    "    phrase = re.sub(r\"\\'s\", \" is\", phrase)\n",
    "    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n",
    "    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n",
    "    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n",
    "    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n",
    "    return phrase "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "def preprocess_sentence(w):\n",
    "    w = w.lower().strip()\n",
    "    # This next line is confusing!\n",
    "    # We normalize unicode data, umlauts will be converted to normal letters\n",
    "    #w = w.replace(\"ß\", \"ss\")\n",
    "    #w = ''.join(c for c in unicodedata.normalize('NFD', w) if unicodedata.category(c) != 'Mn')\n",
    "\n",
    "    # creating a space between a word and the punctuation following it\n",
    "    # eg: \"he is a boy.\" => \"he is a boy .\"\n",
    "    # Reference:- https://stackoverflow.com/questions/3645931/python-padding-punctuation-with-white-spaces-keeping-punctuation\n",
    "    w = re.sub(r\"\\[\\w+\\]\",'', w)\n",
    "    w = \" \".join(re.findall(r\"\\w+\",w))\n",
    "    w = re.sub(r\"([?.!,¿])\", r\" \\1 \", w)\n",
    "    w = re.sub(r'[\" \"]+', \" \", w)\n",
    "\n",
    "    # replacing everything with space except (a-z, A-Z, \".\", \"?\", \"!\", \",\")\n",
    "    w = re.sub(r\"[^a-zA-Z?.!]+\", \" \", w)\n",
    "    w = w.strip()\n",
    "    w = decontracted(w)\n",
    "\n",
    "    # adding a start and an end token to the sentence\n",
    "    # so that the model know when to start and stop predicting.\n",
    "    w = '<start> ' + w + ' <end>'\n",
    "    return w\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After if\n",
      "After first block\n",
      "Greedy sampler set\n",
      "Decoder sampler set\n",
      "Attention mechanism up!\n",
      "Initial state ready\n",
      "Got embedding layer\n"
     ]
    }
   ],
   "source": [
    "#def reply(sentence, preprocess=True):\n",
    "preprocess = True\n",
    "sentence = \"Hi there whats up\"\n",
    "\n",
    "if preprocess:\n",
    "    sentence = preprocess_sentence(sentence)\n",
    "    sentence_tokens = tokenizer.texts_to_sequences([sentence])\n",
    "    input = tf.keras.preprocessing.sequence.pad_sequences(sentence_tokens, maxlen=15, padding='post')\n",
    "else:\n",
    "    input = sentence\n",
    "input = tf.convert_to_tensor(input)\n",
    "\n",
    "print(\"After if\")\n",
    "\n",
    "encoder_hidden = [tf.zeros((1, units)), tf.zeros((1, units))]\n",
    "encoder_output, encoder_h, encoder_c = encoder(input, encoder_hidden)\n",
    "start_token = tf.convert_to_tensor([tokenizer.word_index['<start>']])\n",
    "end_token = tokenizer.word_index['<end>']\n",
    "\n",
    "print(\"After first block\")\n",
    "\n",
    "# This time we use the greedy sampler because we want the word with the highest probability!\n",
    "# We are not generating new text, where a probability sampling would be better\n",
    "greedy_sampler = tfa.seq2seq.GreedyEmbeddingSampler(decoder.embedding_layer)\n",
    "print(\"Greedy sampler set\")\n",
    "# Instantiate a BasicDecoder object\n",
    "decoder_instance = tfa.seq2seq.BasicDecoder(cell=decoder.attention_cell, # N\n",
    "                                            sampler=greedy_sampler, output_layer=decoder.output_layer)\n",
    "print(\"Decoder sampler set\")\n",
    "# Setup Memory in decoder stack\n",
    "decoder.attention_mechanism.setup_memory(encoder_output) # N\n",
    "\n",
    "print(\"Attention mechanism up!\")\n",
    "# set decoder_initial_state\n",
    "decoder_initial_state = decoder.build_initial_state(batch_size=1, encoder_state=[encoder_h, encoder_c]) # N\n",
    "print(\"Initial state ready\")\n",
    "### Since the BasicDecoder wraps around Decoder's rnn cell only, you have to ensure that the inputs to BasicDecoder \n",
    "### decoding step is output of embedding layer. tfa.seq2seq.GreedyEmbeddingSampler() takes care of this. \n",
    "### You only need to get the weights of embedding layer, which can be done by decoder.embedding.variables[0] and pass this callabble to BasicDecoder's call() function\n",
    "\n",
    "decoder_embedding_matrix = decoder.embedding_layer.variables[0]\n",
    "print(\"Got embedding layer\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AttentionWrapperState(cell_state=[<tf.Tensor: shape=(1, 1024), dtype=float32, numpy=\n",
       "array([[ 1.9493668e-03,  1.6567821e-02, -2.3841590e-03, ...,\n",
       "         6.5299624e-05, -2.3378809e-04,  2.0499024e-01]], dtype=float32)>, <tf.Tensor: shape=(1, 1024), dtype=float32, numpy=\n",
       "array([[ 0.02606296,  0.15195727, -0.01356386, ...,  0.00397725,\n",
       "        -0.0104762 ,  0.82305944]], dtype=float32)>], attention=<tf.Tensor: shape=(1, 1024), dtype=float32, numpy=array([[0., 0., 0., ..., 0., 0., 0.]], dtype=float32)>, alignments=<tf.Tensor: shape=(1, 15), dtype=float32, numpy=\n",
       "array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
       "      dtype=float32)>, alignment_history=(), attention_state=<tf.Tensor: shape=(1, 15), dtype=float32, numpy=\n",
       "array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
       "      dtype=float32)>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_initial_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs, _, _ = decoder_instance(decoder_embedding_matrix, start_tokens = start_token, end_token= end_token, initial_state=decoder_initial_state)\n",
    "print(\"Done\")\n",
    "result_sequence  = outputs.sample_id.numpy()\n",
    "#return tokenizer.sequences_to_texts(result_sequence)[0]\n",
    "\n",
    "print(return_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "\n",
    "batch_size = 4\n",
    "hidden_size = 32\n",
    "vocab_size = 64\n",
    "start_token_id = 1\n",
    "end_token_id = 2\n",
    "\n",
    "embedding_layer = tf.keras.layers.Embedding(vocab_size, hidden_size)\n",
    "decoder_cell = tf.keras.layers.LSTMCell(hidden_size)\n",
    "output_layer = tf.keras.layers.Dense(vocab_size)\n",
    "\n",
    "sampler = tfa.seq2seq.GreedyEmbeddingSampler(embedding_layer)\n",
    "decoder = tfa.seq2seq.BasicDecoder(\n",
    "    decoder_cell, sampler, output_layer, maximum_iterations=10\n",
    ")\n",
    "\n",
    "start_tokens = tf.fill([batch_size], start_token_id)\n",
    "initial_state = decoder_cell.get_initial_state(batch_size=batch_size, dtype=tf.float32)\n",
    "final_output, final_state, final_lengths = decoder(\n",
    "    None, start_tokens=start_tokens, end_token=end_token_id, initial_state=initial_state\n",
    ")\n",
    "\n",
    "print(final_output.sample_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
