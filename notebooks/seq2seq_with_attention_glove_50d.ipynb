{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "Xl3J1fhNaOLg"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import tensorflow_addons as tfa\n",
    "from sklearn.model_selection import train_test_split\n",
    "from bs4 import BeautifulSoup\n",
    "import pickle as pkl\n",
    "import unicodedata\n",
    "import spacy\n",
    "from spacy_langdetect import LanguageDetector\n",
    "import time\n",
    "import os\n",
    "import sys\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y0-9bRBeg5Gb"
   },
   "outputs": [],
   "source": [
    "human_lines = []\n",
    "robot_lines = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "__-Tzm_MabXk"
   },
   "outputs": [],
   "source": [
    "data_path_human = \"/content/drive/MyDrive/seminar/seminar/rDany/human_text.txt\"\n",
    "data_path_robot = \"/content/drive/MyDrive/seminar/seminar/rDany/robot_text.txt\"\n",
    "\n",
    "\n",
    "with open(data_path_human, \"r\") as f:\n",
    "    human_lines += f.read().split(\"\\n\")\n",
    "    \n",
    "with open(data_path_robot, \"r\") as f:\n",
    "    robot_lines += f.read().split(\"\\n\")\n",
    "print(human_lines[1])\n",
    "print(robot_lines[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Xoltnv6_eKgV"
   },
   "outputs": [],
   "source": [
    "with open(\"/content/drive/MyDrive/seminar/seminar/input_docs.pkl\", \"rb\") as handle:\n",
    "  human_lines += pkl.load(handle)\n",
    "with open(\"/content/drive/MyDrive/seminar/seminar/target_docs.pkl\", \"rb\") as handle:\n",
    "  robot_lines += pkl.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YuecJHi6QN_r"
   },
   "outputs": [],
   "source": [
    "\n",
    "# function to remove accented characters\n",
    "def remove_accented_chars(text):\n",
    "    new_text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "    return new_text\n",
    "\n",
    "for i, line in tqdm(enumerate(human_lines)):\n",
    "  human_lines[i] = remove_accented_chars(line)\n",
    "\n",
    "for i, line in tqdm(enumerate(robot_lines)):\n",
    "  robot_lines[i] = remove_accented_chars(line) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IofOA9zbT6Vg"
   },
   "outputs": [],
   "source": [
    "english_human_lines = []\n",
    "english_robot_lines = []\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BXuDtXRuTLRJ"
   },
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "nlp.add_pipe(LanguageDetector(), name=\"language_detector\", last=True)\n",
    "\n",
    "for i in tqdm(range(len(human_lines))):\n",
    "  human_doc = nlp(human_lines[i])\n",
    "  robot_doc = nlp(robot_lines[i])\n",
    "\n",
    "  if human_doc._.language['language'] == 'en' and robot_doc._.language['language'] == 'en':\n",
    "    english_human_lines.append(human_lines[i])\n",
    "    english_robot_lines.append(robot_lines[i])\n",
    "print(len(english_human_lines), len(english_robot_lines))\n",
    "human_lines = english_human_lines\n",
    "robot_lines = english_robot_lines\n",
    "del english_human_lines\n",
    "del english_robot_lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QGWdz-5Hd9BO"
   },
   "outputs": [],
   "source": [
    "#with open(\"/content/drive/MyDrive/seminar/english_human.pkl\", \"wb\") as handle:\n",
    "#  pkl.dump(human_lines, handle)\n",
    "\n",
    "#with open(\"/content/drive/MyDrive/seminar/english_robot.pkl\", \"wb\") as handle:\n",
    "#  pkl.dump(robot_lines, handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hljlE0xvgB99"
   },
   "outputs": [],
   "source": [
    "!ls /content/drive/MyDrive/seminar\n",
    "!mv /content/drive/MyDrive/seminar/english_human\\ \\(1\\).pkl /content/drive/MyDrive/seminar/english_human.pkl\n",
    "!mv /content/drive/MyDrive/seminar/english_robot\\ \\(1\\).pkl /content/drive/MyDrive/seminar/english_robot.pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "FM2ZG2IaFV1Y"
   },
   "outputs": [],
   "source": [
    "with open(\"../from_drive/english_human.pkl\", \"rb\") as handle:\n",
    "    human_lines = pkl.load(handle)\n",
    "\n",
    "with open(\"../from_drive/english_robot.pkl\", \"rb\") as handle:\n",
    "    robot_lines = pkl.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "qHaawQs9ugtP"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('What the hell is that sound?', 'Vengeance.')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "human_lines[3002], robot_lines[3002]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "llAsktlLNF5o"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hey I am Yann, how are you and how is it going ? That is interesting: I would love to hear more about it.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def decontracted(phrase):\n",
    "    # specific\n",
    "    phrase = re.sub(r\"won\\'t\", \"will not\", phrase)\n",
    "    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n",
    "\n",
    "    # general\n",
    "    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n",
    "    phrase = re.sub(r\"\\'s\", \" is\", phrase)\n",
    "    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n",
    "    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n",
    "    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n",
    "    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n",
    "    return phrase\n",
    "\n",
    "\n",
    "test = \"Hey I'm Yann, how're you and how's it going ? That's interesting: I'd love to hear more about it.\"\n",
    "print(decontracted(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "HEBiUKBLIpVA"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "64496it [00:02, 26887.23it/s]\n",
      "64496it [00:02, 30573.31it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for i, line in tqdm(enumerate(human_lines)):\n",
    "    human_lines[i] = decontracted(line)\n",
    "\n",
    "for i, line in tqdm(enumerate(robot_lines)):\n",
    "    robot_lines[i] = decontracted(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PlWd7PPGNl4F"
   },
   "source": [
    "# Spell Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "gBWTstoJaj2H"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64496"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "human_lines = [re.sub(r\"\\[\\w+\\]\",'hi',line) for line in human_lines]\n",
    "human_lines = [\" \".join(re.findall(r\"\\w+\",line)) for line in human_lines]\n",
    "robot_lines = [re.sub(r\"\\[\\w+\\]\",'',line) for line in robot_lines]\n",
    "robot_lines = [\" \".join(re.findall(r\"\\w+\",line)) for line in robot_lines]\n",
    "# grouping lines by response pair\n",
    "pairs = list(zip(human_lines,robot_lines))\n",
    "#random.shuffle(pairs)\n",
    "len(pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "lXu7yNobamhA"
   },
   "outputs": [],
   "source": [
    "input_docs = []\n",
    "target_docs = []\n",
    "input_tokens = set()\n",
    "target_tokens = set()\n",
    "\n",
    "for line in pairs:\n",
    "    input_doc, target_doc = line[0], line[1]\n",
    "    # Appending each input sentence to input_docs\n",
    "    input_docs.append(input_doc)\n",
    "    # Splitting words from punctuation  \n",
    "    target_doc = \" \".join(re.findall(r\"[\\w']+|[^\\s\\w]\", target_doc))\n",
    "    # Redefine target_doc below and append it to target_docs\n",
    "    target_doc = '<START> ' + target_doc + ' <END>'\n",
    "    target_docs.append(target_doc)\n",
    "  \n",
    "    # Now we split up each sentence into words and add each unique word to our vocabulary set\n",
    "    for token in re.findall(r\"[\\w']+|[^\\s\\w]\", input_doc):\n",
    "        if token not in input_tokens:\n",
    "            input_tokens.add(token)\n",
    "    for token in target_doc.split():\n",
    "        if token not in target_tokens:\n",
    "            target_tokens.add(token)\n",
    "input_tokens = sorted(list(input_tokens))\n",
    "target_tokens = sorted(list(target_tokens))\n",
    "num_encoder_tokens = len(input_tokens)\n",
    "num_decoder_tokens = len(target_tokens)\n",
    "num_tokens = len(set(input_tokens + target_tokens)) + 2 # [UNK]\n",
    "pairs = list(zip(input_docs, target_docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "kfI0pubtsoPp"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('What the hell is that sound', '<START> Vengeance <END>')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pairs[3002]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "8IGS80-okUYi"
   },
   "outputs": [],
   "source": [
    "vocab_size = 30000 + 1\n",
    "units = 1024\n",
    "embedding_dim = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "wIwgAcz5zCHA"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "tokenizer = Tokenizer(filters='', oov_token=\"<unk>\")\n",
    "tokenizer.fit_on_texts(input_docs + target_docs)\n",
    "\n",
    "tokenizer.num_words = vocab_size\n",
    "input_docs_tokenized = tokenizer.texts_to_sequences(input_docs)\n",
    "target_docs_tokenized = tokenizer.texts_to_sequences(target_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "w5N3AfaWyMTP"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20727, 20727)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_in_docs_tokenized = []\n",
    "final_tar_docs_tokenized = []\n",
    "\n",
    "for i in range(len(input_docs_tokenized)):\n",
    "  if len(input_docs_tokenized[i]) <= 15 and len(target_docs_tokenized[i]) <= 15:\n",
    "    final_in_docs_tokenized.append(input_docs_tokenized[i])\n",
    "    final_tar_docs_tokenized.append(target_docs_tokenized[i])\n",
    "len(final_in_docs_tokenized), len(final_tar_docs_tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qRzIoZUR2K2p"
   },
   "outputs": [],
   "source": [
    "input_docs_tokenized[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "y7VMXQRYB8Bi"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_len = 0\n",
    "for r in final_tar_docs_tokenized:\n",
    "    if len(r) > max_len:\n",
    "        max_len = len(r)\n",
    "  \n",
    "max_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "zzSow6sfa0Wi"
   },
   "outputs": [],
   "source": [
    "\n",
    "X = final_in_docs_tokenized\n",
    "Y = final_tar_docs_tokenized\n",
    "\n",
    "X = tf.keras.preprocessing.sequence.pad_sequences(X, padding='pre')\n",
    "Y = tf.keras.preprocessing.sequence.pad_sequences(Y, padding='pre')\n",
    "\n",
    "del final_in_docs_tokenized\n",
    "del final_tar_docs_tokenized\n",
    "del input_docs\n",
    "del target_docs\n",
    "del input_docs_tokenized\n",
    "del target_docs_tokenized\n",
    "#del human_lines\n",
    "#del robot_lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "wcu_SuRC0QcB"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((20727, 15), (20727, 15))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "gg8uuJuna3O6"
   },
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "JXx3sLHpemYQ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(18654, 15) (2073, 15)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "lVFgTLh1jIQs"
   },
   "outputs": [],
   "source": [
    "BUFFER_SIZE = len(X_train)\n",
    "BATCH_SIZE = 64\n",
    "steps_per_epoch = len(X_train)//BATCH_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "jPZT85Nla5Vs"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4530031it [02:45, 27315.19it/s]\n"
     ]
    }
   ],
   "source": [
    "with open(\"../from_drive/enwiki_20180420_100d.txt\", \"r\") as f:\n",
    "    dict_w2v = {}\n",
    "    problems = []\n",
    "    \n",
    "    for line in tqdm(f):\n",
    "        \n",
    "        tokens = line.split()\n",
    "\n",
    "        try:\n",
    "          word = tokens[0]\n",
    "          vector = np.array(tokens[1:], dtype=np.float32)\n",
    "        except:\n",
    "          pass\n",
    "        \n",
    "        if vector.shape[0] == embedding_dim:\n",
    "            dict_w2v[word] = vector\n",
    "        else:\n",
    "            problems.append({word: vector})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "ge7pv-2zVSOp"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(problems)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "TdfutjjwDHPs"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4529821"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.word_index[list(tokenizer.word_index.keys())[0]]\n",
    "len(dict_w2v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "Jzj2kjQUa7AT"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30000/30000 [00:00<00:00, 70045.16it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29064 936\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "hits = 0\n",
    "misses = 0\n",
    "embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
    "tokens = list(tokenizer.word_index.keys())[:vocab_size-1]\n",
    "\n",
    "for token in tqdm(tokens):\n",
    "    \n",
    "    embedding = dict_w2v.get(token)\n",
    "    \n",
    "    if embedding is not None:\n",
    "        embedding_matrix[tokenizer.word_index[token]] = embedding\n",
    "        hits += 1\n",
    "    else:\n",
    "        misses += 1\n",
    "embedding_matrix[tokenizer.word_index[\"<unk>\"]] = np.random.rand(embedding_dim)\n",
    "print(hits, misses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "l1GEGzrbLnZz"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "167772256\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.getsizeof(dict_w2v))\n",
    "del dict_w2v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "eONR0Tv9cJn-"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hits: 29064\n",
      "Missed: 936\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "46958"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f\"Hits: {hits}\")\n",
    "print(f\"Missed: {misses}\")\n",
    "len(tokenizer.word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "GUPb9vaVgWLt"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "46958"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenizer.word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "gIhaTC9bcM_F"
   },
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices((X_train, Y_train)).shuffle(BUFFER_SIZE).batch(BATCH_SIZE, \n",
    "                                                                                            drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "IPB6XPEkcO5F"
   },
   "outputs": [],
   "source": [
    "def max_len(sentence):\n",
    "    return max(len(s) for s in sentence)\n",
    "\n",
    "max_length_input = max_len(X_train)\n",
    "max_length_output = max_len(Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "4cumw_RZcSHg"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 15)\n",
      "(64, 15)\n"
     ]
    }
   ],
   "source": [
    "for example in dataset.take(1):\n",
    "    example_x, example_y = example\n",
    "    \n",
    "print(example_x.shape) \n",
    "print(example_y.shape) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "6b3lWbbrZghu"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensorflow.python.framework.ops.EagerTensor"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(example_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "oCygKUvfcUCm"
   },
   "outputs": [],
   "source": [
    "class EncoderAttention(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dims, hidden_units):\n",
    "        super().__init__()\n",
    "        self.hidden_units = hidden_units\n",
    "        self.embedding_layer = tf.keras.layers.Embedding(vocab_size, embedding_dims, tf.keras.initializers.Constant(embedding_matrix),\n",
    "                trainable=True)\n",
    "        self.lstm_layer = tf.keras.layers.LSTM(hidden_units, return_sequences=True, \n",
    "                                                     return_state=True ) # We need the lstm outputs \n",
    "                                                                         # to calculate attention!\n",
    "    \n",
    "    def initialize_hidden_state(self): \n",
    "        return [tf.zeros((BATCH_SIZE, self.hidden_units)), \n",
    "                tf.zeros((BATCH_SIZE, self.hidden_units))] \n",
    "                                                               \n",
    "    def call(self, inputs, hidden_state):\n",
    "        embedding = self.embedding_layer(inputs)\n",
    "        output, h_state, c_state = self.lstm_layer(embedding, initial_state = hidden_state)\n",
    "        return output, h_state, c_state\n",
    "\n",
    "\n",
    "encoder = EncoderAttention(vocab_size, embedding_dim, units)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "XC_34Va_cV-u"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 15, 1024)\n",
      "(64, 1024)\n"
     ]
    }
   ],
   "source": [
    "# Test  the encoder\n",
    "sample_initial_state = encoder.initialize_hidden_state()\n",
    "sample_output, sample_h, sample_c = encoder(example_x, sample_initial_state)\n",
    "print(sample_output.shape)\n",
    "print(sample_h.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "_c17LmpfcXsG"
   },
   "outputs": [],
   "source": [
    "\n",
    "class DecoderAttention(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_units):\n",
    "        super().__init__()\n",
    "        \n",
    "        \n",
    "        self.embedding_layer = tf.keras.layers.Embedding(vocab_size, embedding_dim, tf.keras.initializers.Constant(embedding_matrix),\n",
    "                trainable=True)\n",
    "\n",
    "        self.lstm_cell = tf.keras.layers.LSTMCell(hidden_units)\n",
    "\n",
    "        self.sampler = tfa.seq2seq.sampler.TrainingSampler()\n",
    "\n",
    "        self.attention_mechanism = tfa.seq2seq.LuongAttention(hidden_units, memory_sequence_length=BATCH_SIZE*[len(X_train[0])]) #N\n",
    "\n",
    "        self.attention_cell = tfa.seq2seq.AttentionWrapper(cell=self.lstm_cell, # N\n",
    "                                      attention_mechanism=self.attention_mechanism, \n",
    "                                      attention_layer_size=hidden_units)\n",
    "\n",
    "        self.output_layer = tf.keras.layers.Dense(vocab_size)\n",
    "        self.decoder = tfa.seq2seq.BasicDecoder(self.attention_cell, # N\n",
    "                                                sampler=self.sampler, \n",
    "                                                output_layer=self.output_layer)\n",
    "\n",
    "    def build_initial_state(self, batch_size, encoder_state): #N\n",
    "        decoder_initial_state = self.attention_cell.get_initial_state(batch_size=batch_size, dtype=tf.float32)\n",
    "        decoder_initial_state = decoder_initial_state.clone(cell_state=encoder_state)\n",
    "        return decoder_initial_state\n",
    "\n",
    "\n",
    "    def call(self, inputs, initial_state):\n",
    "        embedding = self.embedding_layer(inputs)\n",
    "        outputs, _, _ = self.decoder(embedding, initial_state=initial_state, sequence_length=BATCH_SIZE*[len(Y_train[0])-1])\n",
    "        return outputs\n",
    "\n",
    "decoder = DecoderAttention(vocab_size, embedding_dim, units)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "cauy3GA7cZ_F"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 14, 30001)\n"
     ]
    }
   ],
   "source": [
    "# Test the decoder\n",
    "sample_y = tf.random.uniform((BATCH_SIZE, len(X_train)))\n",
    "decoder.attention_mechanism.setup_memory(sample_output) # Attention needs the last output of the Encoder\n",
    "                                                        # as starting point\n",
    "initial_state = decoder.build_initial_state(BATCH_SIZE, [sample_h, sample_c]) # N\n",
    "\n",
    "\n",
    "sample_decoder_output = decoder(example_y, initial_state)\n",
    "\n",
    "print(sample_decoder_output.rnn_output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "3R-CYQieccKA"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([64, 15, 1024])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "NiIhOi2AceGG"
   },
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    cross_entropy = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
    "    loss = cross_entropy(y_true=real, y_pred=pred)\n",
    "    mask = tf.logical_not(tf.math.equal(real,0))   #output 0 for y=0 else output 1\n",
    "    mask = tf.cast(mask, dtype=loss.dtype)  # mask and loss have to have the same Tensor type\n",
    "    loss = mask * loss\n",
    "    loss = tf.reduce_mean(loss) # you need one loss scalar number for the mini batch\n",
    "    return loss "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "J3GoxLDgcuPq"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "print(tf.config.list_physical_devices(\"GPU\"))\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Bn0oqbEKcfxu"
   },
   "outputs": [],
   "source": [
    "EPOCHS = 10\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    start = time.time()\n",
    "\n",
    "    encoder_hidden = encoder.initialize_hidden_state() # Every epoch we use a zero Tensor matrix\n",
    "    epoch_loss = 0\n",
    "\n",
    "    for (batch, (input, target)) in enumerate(dataset.take(steps_per_epoch)):\n",
    "        with tf.GradientTape() as tape:\n",
    "            # Pass the input through the encoder \n",
    "            encoder_output, encoder_h, encoder_c = encoder(input, encoder_hidden)\n",
    "            decoder_input = target[ : , :-1 ] # Ignore <end> token\n",
    "            real = target[ : , 1: ]         # ignore <start> token\n",
    "            # The encoder output, encoder hidden state and the decoder input\n",
    "            # is passed to the decoder\n",
    "            decoder.attention_mechanism.setup_memory(encoder_output) # N\n",
    "            decoder_initial_state = decoder.build_initial_state(BATCH_SIZE, [encoder_h, encoder_c]) # N\n",
    "            decoder_output = decoder(decoder_input, decoder_initial_state) \n",
    "            logits = decoder_output.rnn_output\n",
    "            batch_loss = loss_function(real, logits)\n",
    "\n",
    "        variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "        gradients = tape.gradient(batch_loss, variables)\n",
    "        optimizer.apply_gradients(zip(gradients, variables))\n",
    "        epoch_loss += batch_loss\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1,\n",
    "                                                       batch,\n",
    "                                                       batch_loss.numpy()))\n",
    "    print('Epoch {} Loss {:.4f}'.format(epoch + 1,\n",
    "                                      epoch_loss / steps_per_epoch))\n",
    "    print('Time {:.4f} sec\\n'.format(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "V6DCbrldkHv2"
   },
   "outputs": [],
   "source": [
    "encoder_model_save_path = \"../chatbot/model_v1/model_2_v3/encoder/weights.ckpt\"\n",
    "decoder_model_save_path = \"../chatbot/model_v1/model_2_v3/decoder/weights.ckpt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OJzSYHaFOJGs"
   },
   "outputs": [],
   "source": [
    "with open(\"/content/drive/MyDrive/seminar/seminar/models/full_models/model_2_v3/tokenizer.pkl\", \"wb\") as handle:\n",
    "  pkl.dump(tokenizer, handle, protocol=pkl.HIGHEST_PROTOCOL)\n",
    "\n",
    "with open(\"/content/drive/MyDrive/seminar/seminar/models/full_models/model_2_v3/encoder_embedding_layer.pkl\", \"wb\") as handle:\n",
    "  pkl.dump(tf.convert_to_tensor(encoder.embedding_layer.variables).numpy(), handle, protocol=pkl.HIGHEST_PROTOCOL)\n",
    "\n",
    "with open(\"/content/drive/MyDrive/seminar/seminar/models/full_models/model_2_v3/decoder_embedding_layer.pkl\", \"wb\") as handle:\n",
    "  pkl.dump(tf.convert_to_tensor(decoder.embedding_layer.variables).numpy(), handle, protocol=pkl.HIGHEST_PROTOCOL)\n",
    "\n",
    "with open(\"/content/drive/MyDrive/seminar/seminar/models/full_models/model_2_v3/hyper_params.yaml\", \"w\") as handle:\n",
    "  hyper_params = \"\"\"\n",
    "    epochs: 70\n",
    "    batch-size: 64\n",
    "    optimizer: adam\n",
    "    vocab-size: 30000\n",
    "  \"\"\"\n",
    "  handle.write(hyper_params)\n",
    "\n",
    "encoder.save_weights(encoder_model_save_path)\n",
    "decoder.save_weights(decoder_model_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "se6XShmMXNgT"
   },
   "outputs": [],
   "source": [
    "tf.compat.v1.enable_eager_execution()\n",
    "tf.convert_to_tensor(encoder.embedding_layer.variables).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "erz4xigFCNk0"
   },
   "outputs": [],
   "source": [
    "#encoder.save_weights(encoder_model_save_path)\n",
    "#decoder.save_weights(decoder_model_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d9MYUTq-Bquy"
   },
   "outputs": [],
   "source": [
    "\n",
    "#encoder.save(encoder_model_save_path, save_format=\"tf\")\n",
    "#decoder.save(decoder_model_save_path, save_format=\"tf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "0IQ5u0wFkE-6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x163685b80>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#encoder = tf.keras.models.load_model(encoder_model_save_path)\n",
    "#decoder = tf.keras.models.load_model(decoder_checkpoint_path)\n",
    "encoder.load_weights(encoder_model_save_path)\n",
    "decoder.load_weights(decoder_model_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "MtcwvktHfKQ7"
   },
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "def preprocess_sentence(w):\n",
    "    w = w.lower().strip()\n",
    "    # This next line is confusing!\n",
    "    # We normalize unicode data, umlauts will be converted to normal letters\n",
    "    #w = w.replace(\"ß\", \"ss\")\n",
    "    #w = ''.join(c for c in unicodedata.normalize('NFD', w) if unicodedata.category(c) != 'Mn')\n",
    "\n",
    "    # creating a space between a word and the punctuation following it\n",
    "    # eg: \"he is a boy.\" => \"he is a boy .\"\n",
    "    # Reference:- https://stackoverflow.com/questions/3645931/python-padding-punctuation-with-white-spaces-keeping-punctuation\n",
    "    w = re.sub(r\"\\[\\w+\\]\",'', w)\n",
    "    w = \" \".join(re.findall(r\"\\w+\",w))\n",
    "    w = re.sub(r\"([?.!,¿])\", r\" \\1 \", w)\n",
    "    w = re.sub(r'[\" \"]+', \" \", w)\n",
    "\n",
    "    # replacing everything with space except (a-z, A-Z, \".\", \"?\", \"!\", \",\")\n",
    "    w = re.sub(r\"[^a-zA-Z?.!]+\", \" \", w)\n",
    "    w = w.strip()\n",
    "    w = decontracted(w)\n",
    "\n",
    "    # adding a start and an end token to the sentence\n",
    "    # so that the model know when to start and stop predicting.\n",
    "    w = '<start> ' + w + ' <end>'\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "8DO2WQ0YfLog"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'oh that here look at this that ai not fun at that <end>'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def reply(sentence, preprocess=True):\n",
    "\n",
    "    if preprocess:\n",
    "        sentence = preprocess_sentence(sentence)\n",
    "        sentence_tokens = tokenizer.texts_to_sequences([sentence])\n",
    "        input = tf.keras.preprocessing.sequence.pad_sequences(sentence_tokens, maxlen=max_length_input, padding='post')\n",
    "    else:\n",
    "        input = sentence\n",
    "    input = tf.convert_to_tensor(input)\n",
    "\n",
    "    encoder_hidden = [tf.zeros((1, units)), tf.zeros((1, units))]\n",
    "    encoder_output, encoder_h, encoder_c = encoder(input, encoder_hidden)\n",
    "    start_token = tf.convert_to_tensor([tokenizer.word_index['<start>']])\n",
    "    end_token = tokenizer.word_index['<end>']\n",
    "\n",
    "    # This time we use the greedy sampler because we want the word with the highest probability!\n",
    "    # We are not generating new text, where a probability sampling would be better\n",
    "    greedy_sampler = tfa.seq2seq.GreedyEmbeddingSampler()\n",
    "\n",
    "    # Instantiate a BasicDecoder object\n",
    "    decoder_instance = tfa.seq2seq.BasicDecoder(cell=decoder.attention_cell, # N\n",
    "                                                sampler=greedy_sampler, output_layer=decoder.output_layer)\n",
    "    # Setup Memory in decoder stack\n",
    "    decoder.attention_mechanism.setup_memory(encoder_output) # N\n",
    "\n",
    "    # set decoder_initial_state\n",
    "    decoder_initial_state = decoder.build_initial_state(batch_size=1, encoder_state=[encoder_h, encoder_c]) # N\n",
    "\n",
    "    ### Since the BasicDecoder wraps around Decoder's rnn cell only, you have to ensure that the inputs to BasicDecoder \n",
    "    ### decoding step is output of embedding layer. tfa.seq2seq.GreedyEmbeddingSampler() takes care of this. \n",
    "    ### You only need to get the weights of embedding layer, which can be done by decoder.embedding.variables[0] and pass this callabble to BasicDecoder's call() function\n",
    "\n",
    "    decoder_embedding_matrix = decoder.embedding_layer.variables[0]\n",
    "\n",
    "    outputs, _, _ = decoder_instance(decoder_embedding_matrix, start_tokens = start_token, end_token= end_token, initial_state=decoder_initial_state)\n",
    "\n",
    "    result_sequence  = outputs.sample_id.numpy()\n",
    "    return tokenizer.sequences_to_texts(result_sequence)[0]\n",
    "reply(\"Hi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "id": "10tAZ4YLslyQ"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.12.0'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfa.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "id": "AGMcKXC-furO"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'no hey hey that is one long from home as long as living <end>'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reply(\"How about we go hiking?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ATXX7zQvfwDE"
   },
   "outputs": [],
   "source": [
    "decoder.embedding_layer.variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lUM48EiefyTd"
   },
   "outputs": [],
   "source": [
    "encoder.load_weights(encoder_checkpoint_path)\n",
    "decoder.load_weights(decoder_checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YmxlA_HEBnrm"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "seq2seq_with_attention_glove_50d.ipynb",
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
